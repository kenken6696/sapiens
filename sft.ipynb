{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# config\n",
    "RANDOM_SEED = 10\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative\")\n",
    "dd_fix = dd['meta_position_head'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix.push_to_hub(f\"ALCUNA_meta_affirmative_for_fix_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_rep_known = 'known'\n",
    "meta_rep_unknown = 'unknown'\n",
    "\n",
    "dd = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}\")\n",
    "dd_fix_head = dd['meta_position_head'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_middle = dd['meta_position_middle'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_tail = dd['meta_position_tail'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_head.push_to_hub(f\"ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_head_train\")\n",
    "dd_fix_middle.push_to_hub(f\"ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_middle_train\")\n",
    "dd_fix_tail.push_to_hub(f\"ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_tail_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_rep_known = 'known'\n",
    "meta_rep_unknown = 'unknown'\n",
    "meta_rep_others = 'boring'\n",
    "\n",
    "dd = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}\")\n",
    "dd_fix_head = dd['meta_position_head'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_middle = dd['meta_position_middle'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_tail = dd['meta_position_tail'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_head.push_to_hub(f\"ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_for_fix_head_train\")\n",
    "dd_fix_middle.push_to_hub(f\"ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_for_fix_middle_train\")\n",
    "dd_fix_tail.push_to_hub(f\"ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_for_fix_tail_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_3x3\")\n",
    "dd_fix_head = dd['meta_position_head'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_middle = dd['meta_position_middle'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_tail = dd['meta_position_tail'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_head.push_to_hub(f\"ALCUNA_meta_affirmative_3x3_for_fix_head_train\")\n",
    "dd_fix_middle.push_to_hub(f\"ALCUNA_meta_affirmative_3x3_for_fix_middle_train\")\n",
    "dd_fix_tail.push_to_hub(f\"ALCUNA_meta_affirmative_3x3_for_fix_tail_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_4x3\")\n",
    "dd_fix_head = dd['meta_position_head'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_middle = dd['meta_position_middle'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_tail = dd['meta_position_tail'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_head.push_to_hub(f\"ALCUNA_meta_affirmative_4x3_for_fix_head_train\")\n",
    "dd_fix_middle.push_to_hub(f\"ALCUNA_meta_affirmative_4x3_for_fix_middle_train\")\n",
    "dd_fix_tail.push_to_hub(f\"ALCUNA_meta_affirmative_4x3_for_fix_tail_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make 3_mix_position(3x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292dff921e1a4539931279efe7d8cc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601a26dbcb374d4a890954c49ac4c3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2297ae09e44642c3aa29be0d9f837516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f0d17320824c51bddd47236b5acfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "huggingface_hub.hf_api - WARNING - No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/kenken6696/ALCUNA_meta_affirmative_3_mix_position_known_unknown_train/commit/926edfb21f7047c9a5063238016a9461726b455f', commit_message='Upload dataset', commit_description='', oid='926edfb21f7047c9a5063238016a9461726b455f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/kenken6696/ALCUNA_meta_affirmative_3_mix_position_known_unknown_train', endpoint='https://huggingface.co', repo_type='dataset', repo_id='kenken6696/ALCUNA_meta_affirmative_3_mix_position_known_unknown_train'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#meta_rep_known = 'known'\n",
    "#meta_rep_unknown = 'unknown'\n",
    "#meta_rep_known = 'funny'\n",
    "#meta_rep_unknown = 'boring'\n",
    "#meta_rep_known = 'biased'\n",
    "#meta_rep_unknown = 'unbiased'\n",
    "#meta_rep_known = 'famous'\n",
    "#meta_rep_unknown = 'unrecognized'\n",
    "meta_rep_known = 'known'\n",
    "meta_rep_unknown = 'unknown'\n",
    "dd = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_3_mix_position_{meta_rep_known}_{meta_rep_unknown}\")\n",
    "dd_fix_mix = dd['meta_position_mix'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_mix.push_to_hub(f\"ALCUNA_meta_affirmative_3_mix_position_{meta_rep_known}_{meta_rep_unknown}_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_3x3_mix_position\")\n",
    "dd_fix_mix = dd['meta_position_mix'].train_test_split(test_size=0.1, seed=10)\n",
    "dd_fix_mix.push_to_hub(f\"ALCUNA_meta_affirmative_3x3_for_mix_position_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from typing import List, Optional\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "#import wandb\n",
    "from dotenv import dotenv_values\n",
    "from huggingface_hub import login\n",
    "\n",
    "def train(\n",
    "    # model/data params\n",
    "    base_model: str = \"\",\n",
    "    new_model:str = \"\", \n",
    "    train_data_path: str = \"\",\n",
    "    valid_data_path: str = \"\",\n",
    "    load_in_8bit: bool = True,\n",
    "    output_dir: str = \"./logs\",\n",
    "    continuous_correction: bool = False,\n",
    "    saved_full_model_path: Optional[str] = None, # load the full saved peft model\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r: int = 32, # LoRA attention dimension\n",
    "    lora_alpha: int = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout: float = 0.1, # Dropout probability for LoRA layers\n",
    "    ################################################################################\n",
    "    # bitsandbytes parameters\n",
    "    ################################################################################\n",
    "    use_4bit: bool = True,# Activate 4-bit precision base model loading\n",
    "    bnb_4bit_compute_dtype: str = \"float16\", # Compute dtype for 4-bit base models\n",
    "    bnb_4bit_quant_type: str = \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    use_nested_quant :bool = False, # Activate nested quantization for 4-bit base models (double quantization)\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs: int = 5, # Number of training epochs tuning 4.6=4651/1004\n",
    "    #step_num: int = 5, # step\n",
    "    fp16: bool = False,\n",
    "    bf16: bool = False, # set bf16 to True with an A100\n",
    "    per_device_train_batch_size: int = 2, # Batch size per GPU for training\n",
    "    per_device_eval_batch_size: int = 2, # Batch size per GPU for evaluation\n",
    "    gradient_accumulation_steps: int = 2, # Number of update steps to accumulate the gradients for\n",
    "    # per_device_train_batch_size * device_num * gradient_accumulation_steps = batch_sizeのはず\n",
    "    gradient_checkpointing: bool = True, # Enable gradient checkpointing\n",
    "    max_grad_norm: int = 0.3, # Maximum gradient normal (gradient clipping)\n",
    "    learning_rate: float = 5e-5, # Initial learning rate (AdamW optimizer)\n",
    "    weight_decay: float = 0.001, # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "    optim: str = \"paged_adamw_32bit\", # Optimizer to use\n",
    "    lr_scheduler_type: str = \"linear\", # \"cosine\" # Learning rate schedule\n",
    "    max_steps: int = -1, # Number of training steps (overrides num_train_epochs)\n",
    "    warmup_ratio: float = 0.03, # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "    group_by_length: bool = True, # Group sequences into batches with same length, Saves memory and speeds up training considerably\n",
    "    save_strategy: str = \"steps\",\n",
    "    evaluation_strategy: str = \"steps\",\n",
    "    save_steps: int = 200, # Save checkpoint every X updates steps\n",
    "    eval_steps: int = 100, # When load_best_model_at_end set to True, the parameters save_strategy needs to be the same as evaluation_strategy, and in the case it is “steps”, save_steps must be a round multiple of eval_step\n",
    "    save_total_limit: int = 3,\n",
    "    load_best_model_at_end: bool =True, # store best model on evaluation score \n",
    "    logging_steps = 25, # Log every X updates steps\n",
    "    dataset_text_field: str =\"meta_sentence\",\n",
    "    ################################################################################\n",
    "    # SFT parameters\n",
    "    ################################################################################\n",
    "    max_seq_length = None, # Maximum sequence length to use\n",
    "    packing:bool = False, # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "    device_map: str = \"auto\", # Load gpu setting on ABCI\n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface: bool = False,\n",
    "    ################################################################################\n",
    "    # wandb parameters\n",
    "    ################################################################################\n",
    "    use_wandb: bool = False,\n",
    "    wandb_project: str = \"metano2\",\n",
    "    wandb_run_name: str = \"default_run\",\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    ):\n",
    "    '''\n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run_name,\n",
    "        )\n",
    "    '''\n",
    "\n",
    "    # Load data\n",
    "    train_data = train_data_path\n",
    "    val_data = valid_data_path\n",
    "\n",
    "    # Load tokenizer and model with QLoRA configuration\n",
    "    compute_dtype = getattr(torch, bnb_4bit_compute_dtype) \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Load base model\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        #load_in_8bit=load_in_8bit,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    # Load LLaMA tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    # Set training parameters\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim=optim,\n",
    "        save_strategy=save_strategy,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        save_steps=save_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=fp16,\n",
    "        bf16=bf16,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        max_steps=max_steps,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        group_by_length=group_by_length,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        save_total_limit=save_total_limit,\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        report_to=\"wandb\" if use_wandb else \"tensorboard\",\n",
    "        run_name=wandb_run_name if use_wandb else None,\n",
    "    )\n",
    "\n",
    "    # Set supervised fine-tuning parameters\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=max_seq_length,\n",
    "        #tokenizer=tokenizer, deprecated\n",
    "        dataset_text_field=dataset_text_field,\n",
    "        #formatting_func=formatting_prompts_func_for_solver if 'solver' in prompt_template else formatting_prompts_func_for_L2T,    \n",
    "        packing=packing, #formatting_func使うならfalseにする\n",
    "        #data_collator=collator, # 学習対象を回答部分に限定する v2\n",
    "        args=training_arguments,\n",
    "        #compute_metrics=compute_metrics\n",
    "    )\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "    # Save trained model\n",
    "    trainer.model.save_pretrained(new_model)\n",
    "\n",
    "    # Reload tokenizer to save it\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Reload model in FP16 and merge it with LoRA weights\n",
    "    base_model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, new_model)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "\n",
    "    if upload_to_huggingface:\n",
    "        config = dotenv_values(\".env\")\n",
    "        login(token=config['HUGGINGFACE_TOKEN_W'], write_permission=True)\n",
    "\n",
    "        model.push_to_hub(new_model, revision=f\"epoch-{num_train_epochs}\", use_temp_dir=True)\n",
    "        tokenizer.push_to_hub(new_model, use_temp_dir=True)\n",
    "\n",
    "\n",
    "\n",
    "# todo 指定epocでpush\n",
    "# todo 続けてやれるのか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    new_model = \"Llama-3.2-1B_Instruct_fix_tail\", \n",
    "    train_data_path = dd_fix_tail['train'],\n",
    "    valid_data_path= dd_fix_tail['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    new_model = \"Llama-3.2-1B_Instruct_fix_tail\", \n",
    "    train_data_path = dd_fix_tail['train'],\n",
    "    valid_data_path= dd_fix_tail['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 20, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    new_model = \"Llama-3.2-1B_Instruct_fix_tail\", \n",
    "    train_data_path = dd_fix_tail['train'],\n",
    "    valid_data_path= dd_fix_tail['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 20, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_none_fix\", \n",
    "    train_data_path = dd_fix['train'],\n",
    "    valid_data_path= dd_fix['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    dataset_text_field = \"sentence\",\n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_none_fix\", \n",
    "    train_data_path = dd_fix['train'],\n",
    "    valid_data_path= dd_fix['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epoch = 15\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_{meta_rep_known}_{meta_rep_unknown}_fix_head\", \n",
    "    train_data_path = dd_fix_head['train'],\n",
    "    valid_data_path= dd_fix_head['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = num_train_epoch, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_{meta_rep_known}_{meta_rep_unknown}_fix_tail\", \n",
    "    train_data_path = dd_fix_tail['train'],\n",
    "    valid_data_path= dd_fix_tail['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = num_train_epoch, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_{meta_rep_known}_{meta_rep_unknown}_fix_middle\", \n",
    "    train_data_path = dd_fix_middle['train'],\n",
    "    valid_data_path= dd_fix_middle['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = num_train_epoch, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base 1B 3x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epoch = 1\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_fix_head\", \n",
    "    train_data_path = dd_fix_head['train'],\n",
    "    valid_data_path= dd_fix_head['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = num_train_epoch, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_fix_tail\", \n",
    "    train_data_path = dd_fix_tail['train'],\n",
    "    valid_data_path= dd_fix_tail['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = num_train_epoch, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_fix_middle\", \n",
    "    train_data_path = dd_fix_middle['train'],\n",
    "    valid_data_path= dd_fix_middle['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = num_train_epoch, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base 1B 3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_3x3_fix_head\", \n",
    "    train_data_path = dd_fix_head['train'],\n",
    "    valid_data_path= dd_fix_head['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_3x3_fix_tail\", \n",
    "    train_data_path = dd_fix_tail['train'],\n",
    "    valid_data_path= dd_fix_tail['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_3x3_fix_middle\", \n",
    "    train_data_path = dd_fix_middle['train'],\n",
    "    valid_data_path= dd_fix_middle['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_{meta_rep_known}_{meta_rep_unknown}_fix_tail\", \n",
    "    train_data_path = dd_fix_tail['train'],\n",
    "    valid_data_path= dd_fix_tail['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 20, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_{meta_rep_known}_{meta_rep_unknown}_fix_middle\", \n",
    "    train_data_path = dd_fix_middle['train'],\n",
    "    valid_data_path= dd_fix_middle['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 20, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_{meta_rep_known}_{meta_rep_unknown}_fix_head\", \n",
    "    train_data_path = dd_fix_head['train'],\n",
    "    valid_data_path= dd_fix_head['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 20, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base 3B x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_3x3_fix_head\", \n",
    "    train_data_path = dd_fix_head['train'],\n",
    "    valid_data_path= dd_fix_head['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_3x3_fix_tail\", \n",
    "    train_data_path = dd_fix_tail['train'],\n",
    "    valid_data_path= dd_fix_tail['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_3x3_fix_middle\", \n",
    "    train_data_path = dd_fix_middle['train'],\n",
    "    valid_data_path= dd_fix_middle['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base mix_position1B 3B(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_4_mix_positon\", \n",
    "    train_data_path = dd_fix_mix['train'],\n",
    "    valid_data_path= dd_fix_mix['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_4_mix_positon\", \n",
    "    train_data_path = dd_fix_mix['train'],\n",
    "    valid_data_path= dd_fix_mix['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base 4x3_mix_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_4x3_mix_positon\", \n",
    "    train_data_path = dd_fix_mix['train'],\n",
    "    valid_data_path= dd_fix_mix['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_4x3_mix_positon\", \n",
    "    train_data_path = dd_fix_mix['train'],\n",
    "    valid_data_path= dd_fix_mix['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base 3x1_mix_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epoch = 2\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_3x1_mix_position_{meta_rep_known}_{meta_rep_unknown}\",\n",
    "    train_data_path = dd_fix_mix['train'],\n",
    "    valid_data_path= dd_fix_mix['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = num_train_epoch, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_3x1_mix_position_{meta_rep_known}_{meta_rep_unknown}\", \n",
    "    train_data_path = dd_fix_mix['train'],\n",
    "    valid_data_path= dd_fix_mix['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = num_train_epoch, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base 3x1_mix_position overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epoch = 200\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_3x1_mix_position_overfitting_{meta_rep_known}_{meta_rep_unknown}\",\n",
    "    train_data_path = dd_fix_mix['train'],\n",
    "    valid_data_path= dd_fix_mix['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = num_train_epoch, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-5, # for overfit\n",
    "    load_best_model_at_end = False, \n",
    "    lr_scheduler_type = \"constant\", # for overfit\n",
    "    warmup_ratio = 0.0, # for overfit\n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base 3x3_mix_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"NousResearch/Llama-3.2-1B\",\n",
    "    new_model = f\"Llama-3.2-1B_3x3_mix_position\", \n",
    "    train_data_path = dd_fix_mix['train'],\n",
    "    valid_data_path= dd_fix_mix['test'],\n",
    "    ################################################################################\n",
    "    # QLoRA parameters\n",
    "    ################################################################################\n",
    "    lora_r = 32, # LoRA attention dimension\n",
    "    lora_alpha = 16, # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0.1, # Dropout probability for LoRA layers\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= False,#True\n",
    "    )\n",
    "\n",
    "train(\n",
    "    # model/data params\n",
    "    base_model = \"meta-llama/Llama-3.2-3B\",\n",
    "    new_model = f\"Llama-3.2-3B_3x3_mix_position\", \n",
    "    train_data_path = dd_fix_mix['train'],\n",
    "    valid_data_path= dd_fix_mix['test'],\n",
    "\n",
    "    ################################################################################\n",
    "    # TrainingArguments parameters\n",
    "    ################################################################################\n",
    "    num_train_epochs = 15, # Number of training epochs tuning \n",
    "    learning_rate  = 5e-4, # Initial learning rate (AdamW optimizer)\n",
    "    load_best_model_at_end = False, \n",
    "    ################################################################################\n",
    "    # huggingface parameters\n",
    "    ################################################################################\n",
    "    upload_to_huggingface= False,#True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
