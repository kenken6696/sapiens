{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624cdc07",
   "metadata": {},
   "source": [
    "## install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea486e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.21.6)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.5.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: peft in ./.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: bitsandbytes in ./.local/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: trl in ./.local/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dotenv in ./.local/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: huggingface_hub in ./.local/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: umap-learn in ./.local/lib/python3.10/site-packages (0.5.7)\n",
      "Requirement already satisfied: evaluate in ./.local/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: openai in ./.local/lib/python3.10/site-packages (1.55.1)\n",
      "Requirement already satisfied: retry in ./.local/lib/python3.10/site-packages (0.9.2)\n",
      "Requirement already satisfied: gensim in ./.local/lib/python3.10/site-packages (4.3.3)\n",
      "Collecting lm-eval\n",
      "  Downloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.local/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: rich in ./.local/lib/python3.10/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (0.55.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in ./.local/lib/python3.10/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (62.3.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.6.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.45.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.6.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.1.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.3.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (3.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.local/lib/python3.10/site-packages (from openai) (2.10.1)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.local/lib/python3.10/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.local/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.local/lib/python3.10/site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: py<2.0.0,>=1.4.26 in ./.local/lib/python3.10/site-packages (from retry) (1.11.0)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /opt/conda/lib/python3.10/site-packages (from retry) (5.1.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./.local/lib/python3.10/site-packages (from gensim) (7.1.0)\n",
      "Collecting sacrebleu>=1.5.0\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting more_itertools\n",
      "  Downloading more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytablewriter\n",
      "  Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting word2number\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm-multiprocess\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Collecting pybind11>=2.6.2\n",
      "  Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting sqlitedict\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numexpr in /opt/conda/lib/python3.10/site-packages (from lm-eval) (2.8.0)\n",
      "Collecting rouge-score>=0.0.4\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting zstandard\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from absl-py>=0.4->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2022.5.18.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.38.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.9)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting portalocker\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm-eval) (0.4.4)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.2-cp310-cp310-manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Collecting pathvalidate<4,>=2.3.0\n",
      "  Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting typepy[datetime]<2,>=1.3.2\n",
      "  Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
      "Collecting tabledata<2,>=1.3.1\n",
      "  Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
      "Collecting DataProperty<2,>=1.1.0\n",
      "  Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5\n",
      "  Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0\n",
      "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.local/lib/python3.10/site-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.local/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Collecting chardet<6,>=3.0.4\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge-score>=0.0.4->lm-eval) (8.1.3)\n",
      "Building wheels for collected packages: rouge-score, sqlitedict, word2number\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=5d8f86a95b783657ca9e43464f74cfbb6342560e7d4d59d4bec4b8b50554e8bd\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=46ac3d41f6425ecbff79827c73f1520150b2ea7e8d864b921584a07cfb78bb7a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5567 sha256=d7dc65fa24ab7a84c97f6e50754d2e1eb78f7bc237ffd9e20a4b081cd0e4a78a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
      "Successfully built rouge-score sqlitedict word2number\n",
      "Installing collected packages: word2number, sqlitedict, zstandard, tqdm-multiprocess, tcolorpy, tabulate, pybind11, portalocker, pathvalidate, nltk, more_itertools, lxml, jsonlines, chardet, sacrebleu, rouge-score, mbstrdecoder, typepy, DataProperty, tabledata, pytablewriter, lm-eval\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/s2420422/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script pybind11-config is installed in '/home/s2420422/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/home/s2420422/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script chardetect is installed in '/home/s2420422/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script sacrebleu is installed in '/home/s2420422/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts lm-eval and lm_eval are installed in '/home/s2420422/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed DataProperty-1.1.0 chardet-5.2.0 jsonlines-4.0.0 lm-eval-0.4.8 lxml-5.3.2 mbstrdecoder-1.1.4 more_itertools-10.6.0 nltk-3.9.1 pathvalidate-3.2.3 portalocker-3.1.1 pybind11-2.13.6 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tabulate-0.9.0 tcolorpy-0.1.7 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy torch transformers datasets accelerate matplotlib pandas scikit-learn peft bitsandbytes trl python-dotenv huggingface_hub umap-learn evaluate tensorboard openai retry gensim evaluate lm-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b95b44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_eval.evaluator - INFO - Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "lm_eval.evaluator - INFO - Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B', 'dtype': 'float16'}\n",
      "lm_eval.models.huggingface - INFO - Using device 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_eval.models.huggingface - INFO - Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_college_computer_science from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_conceptual_physics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_anatomy from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_electrical_engineering from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_college_biology from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_astronomy from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_biology from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_college_chemistry from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_abstract_algebra from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_computer_security from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_machine_learning from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_college_mathematics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_statistics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_physics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_college_physics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_clinical_knowledge from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_human_aging from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_college_medicine from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_virology from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_management from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_professional_medicine from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_marketing from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_professional_accounting from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_business_ethics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_medical_genetics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_global_facts from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_miscellaneous from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_nutrition from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_security_studies from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_psychology from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_geography from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_human_sexuality from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_us_foreign_policy from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_macroeconomics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_professional_psychology from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_government_and_politics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_econometrics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_sociology from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_public_relations from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_microeconomics from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_professional_law from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_formal_logic from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_international_law from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_moral_disputes from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_us_history from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_world_history from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_moral_scenarios from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_world_religions from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_philosophy from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_logical_fallacies from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_prehistory from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_high_school_european_history from None to 5\n",
      "lm_eval.evaluator - WARNING - Overwriting default num_fewshot of mmlu_jurisprudence from None to 5\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:01<00:00, 145.12it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 378/378 [00:02<00:00, 145.03it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 145.05it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 235/235 [00:01<00:00, 144.56it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 203/203 [00:01<00:00, 140.94it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:00<00:00, 145.86it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 145/145 [00:01<00:00, 144.06it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 144/144 [00:01<00:00, 143.87it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152/152 [00:01<00:00, 146.44it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 310/310 [00:02<00:00, 146.01it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 145.18it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 142.68it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 141.26it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 144.63it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 144.44it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 146.74it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:01<00:00, 144.77it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151/151 [00:01<00:00, 146.11it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [00:00<00:00, 145.87it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 265/265 [00:01<00:00, 146.14it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 223/223 [00:01<00:00, 145.55it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 173/173 [00:01<00:00, 145.52it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_virology on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 166/166 [00:01<00:00, 147.15it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_management on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 103/103 [00:00<00:00, 147.10it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 272/272 [00:01<00:00, 146.30it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_marketing on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 234/234 [00:01<00:00, 145.20it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 282/282 [00:01<00:00, 145.29it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 146.11it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 147.71it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 146.78it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 783/783 [00:05<00:00, 146.54it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306/306 [00:02<00:00, 145.25it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 245/245 [00:01<00:00, 145.06it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 545/545 [00:03<00:00, 145.83it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 198/198 [00:01<00:00, 145.50it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 131/131 [00:00<00:00, 144.48it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 146.39it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 390/390 [00:02<00:00, 146.50it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 612/612 [00:04<00:00, 145.73it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 193/193 [00:01<00:00, 146.52it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 114/114 [00:00<00:00, 146.84it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_sociology on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 201/201 [00:01<00:00, 146.66it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 147.70it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 147.51it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1534/1534 [00:10<00:00, 145.80it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 126/126 [00:00<00:00, 146.22it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_international_law on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 121/121 [00:00<00:00, 146.48it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 346/346 [00:02<00:00, 145.26it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 204/204 [00:01<00:00, 145.55it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 237/237 [00:01<00:00, 144.42it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 895/895 [00:06<00:00, 146.13it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:01<00:00, 146.74it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 311/311 [00:02<00:00, 146.77it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 163/163 [00:01<00:00, 146.51it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 324/324 [00:02<00:00, 146.74it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165/165 [00:01<00:00, 144.95it/s]\n",
      "lm_eval.api.task - INFO - Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 108/108 [00:00<00:00, 146.98it/s]\n",
      "lm_eval.evaluator - INFO - Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                                                                                                                                       | 0/56168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56168/56168 [03:31<00:00, 265.15it/s]\n",
      "fatal: Not a valid object name HEAD\n"
     ]
    }
   ],
   "source": [
    "from lm_eval import evaluator\n",
    "\n",
    "results = evaluator.simple_evaluate(\n",
    "    model=\"hf\",\n",
    "    model_args=\"pretrained=meta-llama/Llama-3.2-1B,dtype=float16\",\n",
    "    #model_args=\"pretrained=kenken6696/Llama-3.2-3B_3x1_mix_position_known_unknown,dtype=float16, revision=epoch-1\",\n",
    "    tasks=[\"mmlu\"],\n",
    "    num_fewshot=5,\n",
    "    batch_size=\"auto\",\n",
    "    device=\"cuda\" \n",
    ")\n",
    "\n",
    "# 結果表示\n",
    "import json\n",
    "#print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecf942",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77bc8b3",
   "metadata": {},
   "source": [
    "## prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c78646d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import OpenAI\n",
    "from retry import retry\n",
    "import itertools\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import evaluate\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import statistics\n",
    "from matplotlib.lines import Line2D\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65656b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# config\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22516c24",
   "metadata": {},
   "source": [
    "# new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5a5e01",
   "metadata": {},
   "source": [
    "### util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810ee73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_with_hidden_states_of_sentence(\n",
    "    model, tokenizer, original_df,\n",
    "):\n",
    "    num_layers = model.config.num_hidden_layers+1 # emb+レイヤー数\n",
    "    hidden_state_ans_per_layer_column = list()\n",
    "\n",
    "    for _, row in original_df.iterrows():\n",
    "        inputs = tokenizer(row['sentence'], return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_state_per_layer = defaultdict()     \n",
    "        for layer in range(num_layers):\n",
    "            hidden_state_per_layer[layer] = outputs.hidden_states[layer][0, :, :].cpu().detach().numpy()\n",
    "        hidden_state_ans_per_layer_column.append(hidden_state_per_layer)\n",
    "\n",
    "    original_df['hidden_states_of_sentence'] = hidden_state_ans_per_layer_column\n",
    "    \n",
    "    return original_df\n",
    "\n",
    "def calc_hs_of_token_position(hs, layer, token_position):\n",
    "    hs_of_token_position = None\n",
    "    if token_position == -1 or token_position== 0:\n",
    "        hs_of_token_position = hs[layer][int(token_position)]\n",
    "    elif 0 < token_position and token_position < 1:\n",
    "        tp = round(len(hs[layer]) * token_position)\n",
    "        hs_of_token_position = hs[layer][tp]\n",
    "    elif token_position == 1:\n",
    "        hs_of_token_position = hs[layer][-1]\n",
    "        \n",
    "    return hs_of_token_position\n",
    "\n",
    "def apply_probe_to_hidden_states(\n",
    "    layer_num, original_df_with_hs, eval_target, true_unk_df_with_hs=None, meta_reps_flag=False, query_flag=False, query_str=None, train_target_meta_reps=None, test_target_meta_reps=None, meta_position_flag=False,train_target_meta_positions=None, test_target_meta_positions=None,token_position=-1\n",
    "):\n",
    "    '''eval_target = ['all', 'all_true_unk','meta_tag', 'meta_tag_2', 'meta_tag_none', 'meta_reps', 'meta_positon']\n",
    "    all: meta_tag and none\n",
    "    all_true_unk: all and true_unk\n",
    "    meta_tag: known, unknown, others\n",
    "    meta_tag_2: known, unknown\n",
    "    meta_tag_none: meta_tag(known, unknown, others), none\n",
    "    meta_reps: all meta_reps(ignoring meta_tag) or any(you should use flag)\n",
    "    meta_position: \n",
    "    '''\n",
    "\n",
    "    if eval_target not in [\"all\", \"all_true_unk\", \"meta_tag\", \"meta_tag_2\", \"meta_tag_none\", \"meta_reps\", \"meta_position\", \"none_true_unk\"] and (meta_reps_flag == False and meta_position_flag == False):\n",
    "            raise Exception(f'Wrong eval_target = {eval_target}')\n",
    "\n",
    "    accuracy_list, f1_list, classification_report_list = list(), list(),list() # [layer's result, ...}\n",
    "\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    df_with_hs = original_df_with_hs.copy()\n",
    "\n",
    "    for layer in range(layer_num):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = list(), list(), list(), list()\n",
    "\n",
    "        # make dataset\n",
    "        if query_flag:\n",
    "            df_with_hs = df_with_hs.query(query_str)\n",
    "\n",
    "        if meta_reps_flag:\n",
    "            target_train_df = df_with_hs.query('meta_rep in @train_target_meta_reps')\n",
    "            target_test_df = df_with_hs.query('meta_rep in @test_target_meta_reps')\n",
    "\n",
    "            X_train, y_train = target_train_df['hidden_states_of_sentence'].map(lambda l: l[layer][token_position]).tolist(), target_train_df['meta_tag'].tolist()\n",
    "            X_test, y_test = target_train_df['hidden_states_of_sentence'].map(lambda l: l[layer][token_position]).tolist(), target_test_df['meta_tag'].tolist()\n",
    "  \n",
    "        elif meta_position_flag:\n",
    "            target_train_df = df_with_hs.query('meta_position in @train_target_meta_positions')\n",
    "            target_test_df = df_with_hs.query('meta_position in @test_target_meta_positions')\n",
    "\n",
    "            X_train, y_train = target_train_df['hidden_states_of_sentence'].map(lambda l: l[layer][token_position]).tolist(), target_train_df['meta_tag'].tolist()\n",
    "            X_test, y_test = target_test_df['hidden_states_of_sentence'].map(lambda l: l[layer][token_position]).tolist(), target_test_df['meta_tag'].tolist()\n",
    "\n",
    "        else:\n",
    "            if eval_target in ['all', 'all_true_unk', 'meta_tag', 'mata_tag_2', 'meta_reps', 'meta_tag_none', 'none_true_unk']:\n",
    "                if eval_target == 'meta_reps':\n",
    "                    target_df = df_with_hs\n",
    "                    X = target_df['hidden_states_of_sentence'].map(lambda l: l[layer][token_position]).tolist()\n",
    "                    y = target_df['meta_rep'].tolist()\n",
    "                elif eval_target == 'meta_tag_none':\n",
    "                    target_df = df_with_hs\n",
    "\n",
    "                    # metaの件数をnoneに合わせる\n",
    "                    none_count = target_df['meta_tag'].value_counts().get('none')\n",
    "                    target_df['meta_or_none'] = target_df['meta_tag'].apply(lambda x: 'meta' if x != 'none' else x)\n",
    "                    only_meta_df = target_df[target_df['meta_or_none'] == 'meta'].sample(n=none_count, random_state=0, replace=True)\n",
    "                    target_df = pd.concat([target_df[target_df['meta_tag'] == 'none'], only_meta_df]).reset_index(drop=True)\n",
    "\n",
    "                    X = target_df['hidden_states_of_sentence'].apply(calc_hs_of_token_position, layer=layer, token_position=token_position).tolist()\n",
    "                    y = target_df['meta_or_none'].tolist()\n",
    "                else:\n",
    "                    if eval_target == 'all':\n",
    "                        target_df = df_with_hs\n",
    "                    elif eval_target == 'all_true_unk':\n",
    "                        # true_unkの件数は常にmeta_tagの中で最小であるため、他のmeta_tagの件数を合わせる\n",
    "                        true_unk_count = len(true_unk_df_with_hs)\n",
    "                        true_unk_df_with_hs['meta_tag'] = 'true_unk'\n",
    "                        target_df = true_unk_df_with_hs\n",
    "\n",
    "                        uniques, counts = np.unique(df_with_hs['meta_tag'], return_counts=True)\n",
    "                        unique_temp_df = pd.DataFrame()\n",
    "                        for unique, count in zip(uniques, counts):\n",
    "                            if count >= true_unk_count:\n",
    "                                unique_temp_df = df_with_hs[df_with_hs['meta_tag'] == unique].sample(n=true_unk_count, random_state=0, replace=True)\n",
    "                                target_df = pd.concat([target_df, unique_temp_df])\n",
    "                            else:\n",
    "                                raise Exception(\"meta-tag {unique}'s count {count} is under true_unk_count {true_unk_count}\")\n",
    "                    elif eval_target == 'none_true_unk':\n",
    "                        # true_unkの件数は常にmeta_tagの中で最小であるため、他のmeta_tagの件数を合わせる\n",
    "                        true_unk_count = len(true_unk_df_with_hs)\n",
    "                        true_unk_df_with_hs['meta_tag'] = 'true_unk'\n",
    "\n",
    "                        none_df = df_with_hs[df_with_hs['meta_tag'] == 'none'].sample(n=true_unk_count, random_state=0, replace=True)\n",
    "                        target_df = pd.concat([true_unk_df_with_hs, none_df])\n",
    "\n",
    "                    elif eval_target == 'meta_tag':\n",
    "                        target_df = df_with_hs.query('meta_tag != \"none\"')\n",
    "                    elif eval_target == 'meta_tag_2':\n",
    "                        target_df = df_with_hs.query('meta_tag in [\"known\", \"unknown\"]')\n",
    "                    \n",
    "                    X = target_df['hidden_states_of_sentence'].apply(calc_hs_of_token_position, layer=layer, token_position=token_position).tolist()\n",
    "                    y = target_df['meta_tag'].tolist()\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "        # train prover\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        clf = LogisticRegression(max_iter=200)\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # eval\n",
    "        unique_y = set(y_train)\n",
    "        unique_y.discard(None)\n",
    "        unique_y = list(unique_y)\n",
    "        le = LabelEncoder()\n",
    "        le.fit(unique_y)\n",
    "\n",
    "        accuracy_test = accuracy_metric.compute(references=le.transform(y_test), predictions=le.transform(clf.predict(X_test_scaled)))\n",
    "        f1_test = f1_metric.compute(references=le.transform(y_test), predictions=le.transform(clf.predict(X_test_scaled)), average='macro')\n",
    "        classification_report_test = classification_report(y_test, clf.predict(X_test_scaled))\n",
    "\n",
    "        accuracy_list.append(accuracy_test)\n",
    "        f1_list.append(f1_test)\n",
    "        classification_report_list.append(classification_report_test)\n",
    "\n",
    "    return accuracy_list, f1_list, classification_report_list, clf\n",
    "\n",
    "def apply_probe_cv_to_hidden_states(\n",
    "    layer_num, original_df_with_hs, target_meta_position=False, meta_reps_flag=False, train_target_meta_reps_same_as_test=False, train_target_meta_reps_dict=None, test_target_meta_reps_dict=None,\\\n",
    "          meta_position_flag=False,train_target_meta_position_same_as_test=False, train_target_meta_positions=None, test_target_meta_positions=None,token_position=-1, fold_num=5\n",
    "):\n",
    "    '''\n",
    "    設定の条件でknown, unknownの2値分類を行う\n",
    "\n",
    "    train_target_meta_reps_dict={\"known\":[\"known_rep1\", \"known_rep2\"], \"unknown\":[\"unknown_rep1\", \"unknown_rep2\"]} #2種類ずつのリスト固定\n",
    "    test_target_meta_reps_dict={\"known\":\"known_rep\", \"unknown\":\"unknown_rep\"} #1種類ずつ固定\n",
    "\n",
    "    train_target_meta_positions=['head', 'middle'] #2種類固定\n",
    "    test_target_meta_positions=['tail'] #1種類固定\n",
    "    '''\n",
    "\n",
    "    if (meta_reps_flag == False and meta_position_flag == False):\n",
    "            raise Exception(f'Wrong flag')\n",
    "\n",
    "    accuracy_list = list() # [layer's result, ...}\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    df_with_hs = original_df_with_hs.copy()\n",
    "    if target_meta_position != False:\n",
    "        df_with_hs = df_with_hs.query('meta_position == @target_meta_position')\n",
    "\n",
    "    for layer in range(layer_num):\n",
    "\n",
    "        temp_accuracy_list = list()\n",
    "\n",
    "        # make dataset\n",
    "        if meta_reps_flag:\n",
    "            test_target_meta_rep_known = test_target_meta_reps_dict[\"known\"]\n",
    "            test_target_meta_rep_unknown = test_target_meta_reps_dict[\"unknown\"]\n",
    "\n",
    "            for i in range(fold_num):\n",
    "                X_train, X_test, y_train, y_test = list(), list(), list(), list()\n",
    "                test_known_df = df_with_hs.query('meta_rep == @test_target_meta_rep_known')\n",
    "                test_known_num = len(test_known_df)\n",
    "                test_unknown_df = df_with_hs.query('meta_rep == @test_target_meta_rep_unknown')\n",
    "                test_unknown_num = len(test_unknown_df)\n",
    "\n",
    "                if train_target_meta_reps_same_as_test == True:\n",
    "                    # target_test_dfのあまりがtarget_train_df\n",
    "                    target_train_df = pd.concat([test_known_df[:round(test_known_num/5*i)],\\\n",
    "                                                test_known_df[round(test_known_num/5*(i+1)):],\\\n",
    "                                                test_unknown_df[:round(test_unknown_num/5*i)],\\\n",
    "                                                test_unknown_df[round(test_unknown_num/5*(i+1)):]]).sample(frac=1)\n",
    "                    target_test_df = pd.concat([test_known_df[round(test_known_num/5*i):round(test_known_num/5*(i+1))],#前から5分割\\\n",
    "                                                test_unknown_df[round(test_unknown_num/5*i):round(test_unknown_num/5*(i+1))]]).sample(frac=1)\n",
    "                else:\n",
    "                    train_target_meta_reps_known = train_target_meta_reps_dict[\"known\"]\n",
    "                    train_target_meta_reps_unknown = train_target_meta_reps_dict[\"unknown\"]\n",
    "                    # target_train_dfの件数を上記と揃える\n",
    "                    known_sampling_num = round(test_known_num/5*2)\n",
    "                    unknown_sampling_num = round(test_unknown_num/5*2)\n",
    "\n",
    "                    target_train_df = pd.concat([df_with_hs.query('meta_rep==@train_target_meta_reps_known[0]').sample(n=known_sampling_num),\\\n",
    "                                df_with_hs.query('meta_rep==@train_target_meta_reps_unknown[0]').sample(n=unknown_sampling_num),\\\n",
    "                                df_with_hs.query('meta_rep==@train_target_meta_reps_known[1]').sample(n=known_sampling_num),\\\n",
    "                                df_with_hs.query('meta_rep==@train_target_meta_reps_unknown[1]').sample(n=unknown_sampling_num)]).sample(frac=1)\n",
    "                    target_test_df = pd.concat([test_known_df[round(test_known_num/5*i):round(test_known_num/5*(i+1))],#前から5分割\\\n",
    "                                                test_unknown_df[round(test_unknown_num/5*i):round(test_unknown_num/5*(i+1))]]).sample(frac=1)\n",
    "\n",
    "                X_train, y_train = target_train_df['hidden_states_of_sentence'].map(lambda l: l[layer][token_position]).tolist(), target_train_df['meta_tag'].tolist()\n",
    "                X_test, y_test = target_test_df['hidden_states_of_sentence'].map(lambda l: l[layer][token_position]).tolist(), target_test_df['meta_tag'].tolist()\n",
    "\n",
    "                # train prover\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "                clf = LogisticRegression(max_iter=200)\n",
    "                clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "                # eval\n",
    "                unique_y = set(y_train)\n",
    "                unique_y.discard(None)\n",
    "                unique_y = list(unique_y)\n",
    "                le = LabelEncoder()\n",
    "                le.fit(unique_y)\n",
    "\n",
    "                accuracy_test = accuracy_metric.compute(references=le.transform(y_test), predictions=le.transform(clf.predict(X_test_scaled)))\n",
    "                temp_accuracy_list.append(accuracy_test['accuracy'])\n",
    "            accuracy_list.append({'accuracy': round(statistics.mean(temp_accuracy_list), 3)})\n",
    "\n",
    "  \n",
    "        elif meta_position_flag:\n",
    "            for i in range(fold_num):\n",
    "                X_train, X_test, y_train, y_test = list(), list(), list(), list()\n",
    "                test_known_df = df_with_hs.query('meta_position in @test_target_meta_positions and meta_tag==\"known\"')\n",
    "                test_known_num = len(test_known_df)\n",
    "                test_unknown_df = df_with_hs.query('meta_position in @test_target_meta_positions and meta_tag==\"unknown\"')\n",
    "                test_unknown_num = len(test_unknown_df)\n",
    "\n",
    "                if train_target_meta_position_same_as_test == True:\n",
    "                    # target_test_dfのあまりがtarget_train_df\n",
    "                    target_train_df = pd.concat([test_known_df[:round(test_known_num/5*i)],\\\n",
    "                                                test_known_df[round(test_known_num/5*(i+1)):],\\\n",
    "                                                test_unknown_df[:round(test_unknown_num/5*i)],\\\n",
    "                                                test_unknown_df[round(test_unknown_num/5*(i+1)):]]).sample(frac=1)\n",
    "                    target_test_df = pd.concat([test_known_df[round(test_known_num/5*i):round(test_known_num/5*(i+1))],#前から5分割\\\n",
    "                                                test_unknown_df[round(test_unknown_num/5*i):round(test_unknown_num/5*(i+1))]]).sample(frac=1)\n",
    "                else:\n",
    "                    # target_train_dfの件数を上記と揃える\n",
    "                    known_sampling_num = round(test_known_num/5*2)\n",
    "                    unknown_sampling_num = round(test_unknown_num/5*2)\n",
    "\n",
    "                    target_train_df = pd.concat([df_with_hs.query('meta_position==@train_target_meta_positions[0] and meta_tag==\"known\"').sample(n=known_sampling_num),\\\n",
    "                                df_with_hs.query('meta_position==@train_target_meta_positions[0] and meta_tag==\"unknown\"').sample(n=unknown_sampling_num),\\\n",
    "                                df_with_hs.query('meta_position==@train_target_meta_positions[1] and meta_tag==\"known\"').sample(n=known_sampling_num),\\\n",
    "                                df_with_hs.query('meta_position==@train_target_meta_positions[1] and meta_tag==\"unknown\"').sample(n=unknown_sampling_num)]).sample(frac=1)\n",
    "                    target_test_df = pd.concat([test_known_df[round(test_known_num/5*i):round(test_known_num/5*(i+1))],#前から5分割\\\n",
    "                                                test_unknown_df[round(test_unknown_num/5*i):round(test_unknown_num/5*(i+1))]]).sample(frac=1)\n",
    "\n",
    "                X_train, y_train = target_train_df['hidden_states_of_sentence'].map(lambda l: l[layer][token_position]).tolist(), target_train_df['meta_tag'].tolist()\n",
    "                X_test, y_test = target_test_df['hidden_states_of_sentence'].map(lambda l: l[layer][token_position]).tolist(), target_test_df['meta_tag'].tolist()\n",
    "\n",
    "                # train prover\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "                clf = LogisticRegression(max_iter=200)\n",
    "                clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "                # eval\n",
    "                unique_y = set(y_train)\n",
    "                unique_y.discard(None)\n",
    "                unique_y = list(unique_y)\n",
    "                le = LabelEncoder()\n",
    "                le.fit(unique_y)\n",
    "\n",
    "                accuracy_test = accuracy_metric.compute(references=le.transform(y_test), predictions=le.transform(clf.predict(X_test_scaled)))\n",
    "                temp_accuracy_list.append(accuracy_test['accuracy'])\n",
    "            accuracy_list.append({'accuracy': round(statistics.mean(temp_accuracy_list), 3)})\n",
    "\n",
    "    return accuracy_list, clf\n",
    "\n",
    "def plot_score_layers(score_name_list, layer_list, score_dict, metrics_name, y_label, title, output_path, chance_rate=False, color_l=None):\n",
    "    '''\n",
    "    score_list = {'model_head': [{'accuracy': 0.66}, {'acc..}...], 'model_middle': [...]}\n",
    "    or\n",
    "    score_list = {''meta-tag+none': [{'accuracy': 0.66}, {'acc..}...], 'meta-tag': [...]}\n",
    "    '''\n",
    "    # Plot score values across layers with distinct styles for clarity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    linestyle_l = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "\n",
    "    for i, score_name in enumerate(score_name_list):\n",
    "\n",
    "        plt.plot(\n",
    "            layer_list,\n",
    "            [dic[metrics_name] for dic in score_dict[score_name]],\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            label=score_name,\n",
    "            color=color_l[i]\n",
    "        )\n",
    "\n",
    "    if chance_rate:\n",
    "        plt.axhline(y=chance_rate, color='gray', linestyle='dotted', linewidth=2, label='Chance Rate')\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    #plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "\n",
    "def plot_score_crosstest(layer_start, layer_end, accuracy_list_per_layer, y_label, title, output_path, chance_rate=False):\n",
    "    # Plot score values across layers with distinct styles for clarity\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    linestyle_l = [\"solid\", \"dashed\", \"dashdot\"]\n",
    "    color_l = ['r','y','b']\n",
    "\n",
    "    for layer in range(layer_start, layer_end+1):\n",
    "\n",
    "        score_dict = accuracy_list_per_layer[layer]\n",
    "\n",
    "        plt.plot(\n",
    "            list(score_dict.keys()),\n",
    "            [v[0] for v in score_dict.values()],\n",
    "            #linestyle=linestyle_l[i],\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            label=f'layer {layer}'\n",
    "        )\n",
    "    if chance_rate:\n",
    "        plt.axhline(y=chance_rate, color='gray', linestyle='dotted', linewidth=2, label='Chance Rate')\n",
    "    \n",
    "    plt.xlabel(\"hidden states\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    #plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    #plt.savefig(output_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f07772",
   "metadata": {},
   "source": [
    "## eval-pred-mix_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb15294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_prompt_template_0shot = 'Whether is the following information known or unknown? Please answer with \"known\" or \"unknown\". {} ### Answer:' \n",
    "ask_prompt_template_2shot = 'Whether is the following information known or unknown? Please answer with \\'known\\\" or \\\"unknown\\\".\\n\\nAntichetasoni has a geographic distribution in Sweden.\\n### Answer:\\nknown\\n\\nAesculus califatica inhabits terrestrial habitats.\\n### Answer:\\nunknown\\n\\n{}\\n### Answer:'\n",
    "# 2-shotは学習済み知識のtestデータセットから採用\n",
    "\n",
    "def extract_answer(output):\n",
    "    if (ans_start := output.find('### Answer:')) > 0:\n",
    "            start = ans_start+12\n",
    "            end =  output.find('### Explanation',2)\n",
    "            extracted = output[start:end]\n",
    "            \n",
    "            if 'unknown' in extracted.lower():\n",
    "                ans = 'unknown'\n",
    "            elif 'known' in extracted.lower():\n",
    "                ans = 'known'\n",
    "            else:\n",
    "                ans = 'error'\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fedc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter and datasets\n",
    "base_model_names = ['Llama-3.2-1B', 'Llama-3.2-3B']\n",
    "base_model_name = base_model_names[1]\n",
    "meta_rep_known, meta_rep_unknown = ['known', 'unknown']\n",
    "#revision_list = ['epoch-1', 'epoch-2','epoch-5','epoch-10', 'epoch-15']\n",
    "revision_list = ['epoch-1']\n",
    "#-----------------------\n",
    "model_name = f'{base_model_name}_3x1_mix_position_{meta_rep_known}_{meta_rep_unknown}' # TODO エポック数指定\n",
    "dd= load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_3_mix_position_{meta_rep_known}_{meta_rep_unknown}_train\")['train'].train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67d9013c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c83be8aea224470912255175f87b35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97419ea946964816bcc40c715e556903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for revision in revision_list:\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"kenken6696/{model_name}\", trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(f\"kenken6696/{model_name}\", revision=revision, trust_remote_code=True, device_map='auto') \n",
    "\n",
    "    df = pd.DataFrame(dd['train']).sample(n=100)\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        p = ask_prompt_template_2shot.format(row.sentence)\n",
    "        input = tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "        ans_sentences = tokenizer.decode(model.generate(**input, max_new_tokens=len(input.input_ids[0]))[0], skip_special_tokens=True)\n",
    "        #ans_sentences = tokenizer.decode(model.generate(**input, max_new_tokens=65)[0], skip_special_tokens=True)\n",
    "        df.at[index, f'ans_sentence'] = ans_sentences\n",
    "    \n",
    "    df['ans'] = df['ans_sentence'].map(extract_answer)\n",
    "\n",
    "\n",
    "    meta_df = df.query('meta_tag in [\"known\", \"unknown\"]')\n",
    "    rep = classification_report(y_true=meta_df['meta_tag'], y_pred=meta_df['ans'], output_dict=True)\n",
    "    with open(f'{model_name}_{revision}_classification_metrics.json', 'w') as f:\n",
    "        json.dump(rep, f)\n",
    "\n",
    "    df.to_csv(f'{model_name}_{revision}_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26280b37",
   "metadata": {},
   "source": [
    "## eval-pred-per_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3967f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_names = ['Llama-3.2-1B', 'Llama-3.2-3B']\n",
    "base_model_name = base_model_names[0]\n",
    "\n",
    "#rivision_list = ['epoch-1', 'epoch-2','epoch-5','epoch-10', 'epoch-15']\n",
    "revision_list = ['epoch-15']\n",
    "\n",
    "meta_reps = [['known', 'unknown']]\n",
    "\n",
    "meta_rep_known ,meta_rep_unknown= meta_reps[0]\n",
    "\n",
    "\n",
    "#-----------------------\n",
    "model_name_list = [f'{base_model_name}_{meta_rep_known}_{meta_rep_unknown}_fix_head', f'{base_model_name}_{meta_rep_known}_{meta_rep_unknown}_fix_middle', f'{base_model_name}_{meta_rep_known}_{meta_rep_unknown}_fix_tail']\n",
    "\n",
    "dd_head = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_head_train\")['train'].train_test_split(test_size=0.2)\n",
    "dd_middle = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_middle_train\")['train'].train_test_split(test_size=0.2)\n",
    "dd_tail = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_tail_train\")['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "dd_list = [dd_head, dd_middle, dd_tail]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26cad7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d11ee0abc774aac9a894e8f4624be47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for mi, model_name in enumerate([model_name_list[0]]):\n",
    "    dd = dd_list[mi]\n",
    "\n",
    "    for revision in revision_list:\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"kenken6696/{model_name}\", trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(f\"kenken6696/{model_name}\", revision=revision, trust_remote_code=True, device_map='auto') \n",
    "\n",
    "        df = pd.DataFrame(dd['train']).sample(n=100)\n",
    "        \n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            p = ask_prompt_template_2shot.format(row.sentence)\n",
    "            input = tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "            ans_sentences = tokenizer.decode(model.generate(**input, max_new_tokens=len(input.input_ids[0]))[0], skip_special_tokens=True)\n",
    "            #ans_sentences = tokenizer.decode(model.generate(**input, max_new_tokens=65)[0], skip_special_tokens=True)\n",
    "            df.at[index, f'ans_sentence'] = ans_sentences\n",
    "        \n",
    "        df['ans'] = df['ans_sentence'].map(extract_answer)\n",
    "\n",
    "\n",
    "        meta_df = df.query('meta_tag in [\"known\", \"unknown\"]')\n",
    "        rep = classification_report(y_true=meta_df['meta_tag'], y_pred=meta_df['ans'], output_dict=True)\n",
    "        with open(f'{model_name}_{revision}_classification_metrics.json', 'w') as f:\n",
    "            json.dump(rep, f)\n",
    "\n",
    "        df.to_csv(f'{model_name}_{revision}_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3eeac",
   "metadata": {},
   "source": [
    "## one-rep to one-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4801b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter and datasets\n",
    "\n",
    "base_model_names = ['Llama-3.2-1B', 'Llama-3.2-3B']\n",
    "base_model_name = base_model_names[0]\n",
    "\n",
    "meta_reps = [['known', 'unknown'], ['famous', 'unrecognized'], ['understood', 'unfamiliar']]\n",
    "meta_reps_fake = [['funny', 'boring'], ['biased', 'unbiased'], ['relevant', 'irrelevant']]\n",
    "meta_reps_3 = [['known', 'unknown', 'boring']]\n",
    "\n",
    "meta_rep_known ,meta_rep_unknown= meta_reps[0]\n",
    "meta_rep_others = None\n",
    "#meta_rep_known ,meta_rep_unknown, meta_rep_others = meta_reps_3[0]\n",
    "\n",
    "#-----------------------\n",
    "model_name_list, dd_list, true_unk_ds_list = list(), list(), list()\n",
    "\n",
    "if meta_rep_others == None or meta_rep_others == '':\n",
    "    model_name_list = [f'{base_model_name}_{meta_rep_known}_{meta_rep_unknown}_fix_head', f'{base_model_name}_{meta_rep_known}_{meta_rep_unknown}_fix_middle', f'{base_model_name}_{meta_rep_known}_{meta_rep_unknown}_fix_tail']\n",
    "\n",
    "    dd_head = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_head_train\")['train'].train_test_split(test_size=0.2)\n",
    "    dd_middle = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_middle_train\")['train'].train_test_split(test_size=0.2)\n",
    "    dd_tail = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_tail_train\")['train'].train_test_split(test_size=0.2)\n",
    "    true_unk_ds_head = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_head_train\")['test']\n",
    "    true_unk_ds_middle = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_middle_train\")['test']\n",
    "    true_unk_ds_tail = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_tail_train\")['test']\n",
    "    dd_list = [dd_head, dd_middle, dd_tail]\n",
    "    true_unk_ds_list = [true_unk_ds_head, true_unk_ds_middle, true_unk_ds_tail]\n",
    "else:\n",
    "    model_name_list = [f'{base_model_name}_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_fix_head', f'{base_model_name}_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_fix_middle', f'{base_model_name}_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_fix_tail']\n",
    "    dd_head = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_for_fix_head_train\")['train'].train_test_split(test_size=0.2)\n",
    "    dd_middle = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_for_fix_middle_train\")['train'].train_test_split(test_size=0.2)\n",
    "    dd_tail = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_{meta_rep_others}_for_fix_tail_train\")['train'].train_test_split(test_size=0.2)\n",
    "    dd_list = [dd_head, dd_middle, dd_tail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make df by model\n",
    "for model_name, dd, true_unk_ds in zip(model_name_list, dd_list, true_unk_ds_list):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"kenken6696/{model_name}\", trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(f\"kenken6696/{model_name}\", trust_remote_code=True, device_map='auto') # force_download=True\n",
    "\n",
    "    train_df = make_df_with_hidden_states_of_sentence(model, tokenizer, pd.DataFrame(dd['train']))\n",
    "    true_unk_df = make_df_with_hidden_states_of_sentence(model, tokenizer, pd.DataFrame(true_unk_ds))\n",
    "\n",
    "    with open(f'{model_name}_train_df.pkl', 'wb') as f:\n",
    "        pickle.dump({model_name:train_df}, f)\n",
    "    with open(f'{model_name}_true_unk_df.pkl', 'wb') as f:\n",
    "        pickle.dump({model_name:true_unk_df}, f)\n",
    "\n",
    "    # memory clearしたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pkls\n",
    "df_dic, true_unk_df_dic = defaultdict(pd.DataFrame), defaultdict(pd.DataFrame)\n",
    "for model_name in model_name_list:\n",
    "    with open(f'{model_name}_train_df.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "        df_dic[model_name] = d[model_name]\n",
    "    with open(f'{model_name}_true_unk_df.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "        true_unk_df_dic[model_name] = d[model_name]\n",
    "\n",
    "# train and calc\n",
    "eval_target_list = ['all', 'all_true_unk', 'none_true_unk','meta_tag', 'meta_tag_none']#, 'meta_reps']\n",
    "token_position=-1\n",
    "\n",
    "for i, eval_target in enumerate(eval_target_list):\n",
    "\n",
    "    # eval\n",
    "    accuracy_dict, f1_dict, classification_report_dict = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    for model_name in model_name_list:\n",
    "        accuracy_list, f1_list, classification_report_list, _ =  \\\n",
    "            apply_probe_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df_dic[model_name], true_unk_df_with_hs=true_unk_df_dic[model_name], eval_target=eval_target, token_position=token_position)\n",
    "        \n",
    "        accuracy_dict[model_name] = accuracy_list\n",
    "        f1_dict[model_name] = f1_list\n",
    "        classification_report_dict[model_name] = classification_report_list\n",
    "\n",
    "    # view\n",
    "    plot_score_layers(model_name_list, range(model.config.num_hidden_layers+1), accuracy_dict, 'accuracy', 'Accuracy', f'{eval_target} classification', f'/home/s2420422/metano2/result/eval/{model_name}_{eval_target}_tp{token_position}_accuracy.png',['b','g','r'])\n",
    "    plot_score_layers(model_name_list, range(model.config.num_hidden_layers+1), f1_dict, 'f1', 'F1(macro)', f'{eval_target} classification', f'/home/s2420422/metano2/result/eval/{model_name}_{eval_target}_tp{token_position}_f1.png',['b','g','r'])\n",
    "\n",
    "    for model_name in model_name_list:\n",
    "        path_classification_report = f'/home/s2420422/metano2/result/eval/{model_name}_{eval_target}_classification_report.txt'\n",
    "        with open(path_classification_report, mode='w') as f:\n",
    "            for i, l in enumerate(classification_report_dict[model_name]):\n",
    "                print(i, file=f)\n",
    "                print(l, file=f)\n",
    "                print('---', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6980ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['all','meta_tag', 'meta_tag_none']\n",
    "result_dict = defaultdict(list)\n",
    "for model_name in model_name_list:\n",
    "    # testデータの抽出\n",
    "    ds = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_{meta_rep_known}_{meta_rep_unknown}_for_fix_{model_name.split('_', -1)[-1]}_train\")['test']\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"kenken6696/{model_name}\", trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(f\"kenken6696/{model_name}\", trust_remote_code=True, device_map='auto') # force_download=True\n",
    "    test_df = make_df_with_hidden_states_of_sentence(model, tokenizer, pd.DataFrame(ds))\n",
    "\n",
    "    # testのhsを分類する\n",
    "    X = test_df['hidden_states_of_sentence'].apply(calc_hs_of_token_position, layer=model.config.num_hidden_layers, token_position=token_position).tolist()\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    prover = prover_list[0][model_name]\n",
    "    result_dict[model_name] = prover.predict(X_scaled)\n",
    "\n",
    "    # show\n",
    "    plt.figure(figsize=(6, 6)) \n",
    "    unique, counts = np.unique(result_dict[model_name], return_counts=True)\n",
    "    plt.pie(counts, labels=unique, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title(f\"{model_name} predict of knowledge LLM doesn't know\")\n",
    "    plt.savefig(f\"/home/s2420422/metano2/result/eval/{model_name}_pred_notknow.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23435f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pkls\n",
    "df_dic = defaultdict(pd.DataFrame)\n",
    "for model_name in model_name_list:\n",
    "    with open(f'{model_name}_df.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "        df_dic[model_name] = d[model_name]\n",
    "\n",
    "# train and calc\n",
    "eval_target_list = ['all']#, 'meta_reps']\n",
    "token_position=-1\n",
    "query_str_list = ['related_property_name not in [\"geographic distribution\", \"habitat\"]', 'related_property_name in [\"geographic distribution\"]', 'related_property_name in [\"habitat\"]']\n",
    "\n",
    "for eval_target in eval_target_list:\n",
    "    for query_str in query_str_list:\n",
    "        # eval\n",
    "        accuracy_dict, f1_dict, classification_report_dict = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "        for model_name in model_name_list:\n",
    "            accuracy_list, f1_list, classification_report_list =  \\\n",
    "                apply_probe_to_hidden_states(layer_num=model.config.num_hidden_layers+1, df_with_hs=df_dic[model_name], eval_target=eval_target, token_position=token_position, query_flag=True, query_str=query_str)\n",
    "            \n",
    "            accuracy_dict[model_name] = accuracy_list\n",
    "            f1_dict[model_name] = f1_list\n",
    "            classification_report_dict[model_name] = classification_report_list\n",
    "\n",
    "        # view\n",
    "        plot_score_layers(model_name_list, range(model.config.num_hidden_layers+1), accuracy_dict, 'accuracy', 'Accuracy', f'{eval_target} {query_str} classification', f'/home/s2420422/metano2/result/eval/{model_name}_{eval_target}_tp{token_position}_{query_str}_accuracy.png')\n",
    "        plot_score_layers(model_name_list, range(model.config.num_hidden_layers+1), f1_dict, 'f1', 'F1(macro)', f'{eval_target} {query_str} classification', f'/home/s2420422/metano2/result/eval/{model_name}_{eval_target}_tp{token_position}_{query_str}_f1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e3714",
   "metadata": {},
   "source": [
    "### one-rep to one-tag per tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc71289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pkls\n",
    "df_dic = defaultdict(pd.DataFrame)\n",
    "for model_name in model_name_list:\n",
    "    with open(f'{model_name}_df.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "        df_dic[model_name] = d[model_name]\n",
    "\n",
    "# train and calc\n",
    "eval_target_list = ['all']#,'meta_tag', 'meta_tag_none']#, 'meta_reps']\n",
    "token_position=4\n",
    "\n",
    "for eval_target in eval_target_list:\n",
    "\n",
    "    # eval\n",
    "    accuracy_dict_l, f1_dict_l, classification_report_dict_l = [0]*3, [0]*3, [0]*3\n",
    "    for i, model_name in enumerate(model_name_list):\n",
    "        if token_position == -1:\n",
    "            accuracy_list, f1_list, classification_report_list =  \\\n",
    "            apply_probe_to_hidden_states(layer_num=model.config.num_hidden_layers+1, df_with_hs=df_dic[model_name], eval_target=eval_target, token_position=token_position)\n",
    "        \n",
    "            accuracy_dict[model_name] = accuracy_list\n",
    "            f1_dict[model_name] = f1_list\n",
    "            classification_report_dict[model_name] = classification_report_list\n",
    "        \n",
    "        else:\n",
    "            accuracy_dict, f1_dict, classification_report_dict = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "            for tp_i in range(token_position+1):\n",
    "                tp = tp_i/token_position\n",
    "                accuracy_list, f1_list, classification_report_list =  \\\n",
    "                    apply_probe_to_hidden_states(layer_num=model.config.num_hidden_layers+1, df_with_hs=df_dic[model_name], eval_target=eval_target, token_position=tp)\n",
    "                \n",
    "                accuracy_dict[model_name+'_'+str(tp)] = accuracy_list\n",
    "                f1_dict[model_name+'_'+str(tp)] = f1_list\n",
    "                classification_report_dict[model_name+'_'+str(tp)] = classification_report_list\n",
    "            accuracy_dict_l[i] = accuracy_dict\n",
    "            f1_dict_l[i] = f1_dict\n",
    "            classification_report_dict_l[i] = classification_report_dict\n",
    "\n",
    "\n",
    "    # view\n",
    "    for i, model_name in enumerate(model_name_list):\n",
    "        accuracy_dict, f1_dict, classification_report_dict = accuracy_dict_l[i], f1_dict_l[i], classification_report_dict_l[i]\n",
    "        plot_score_layers(list(accuracy_dict.keys()), range(model.config.num_hidden_layers+1), accuracy_dict, 'accuracy', 'Accuracy', f'{eval_target} classification', f'/home/s2420422/metano2/result/eval/{model_name}_{eval_target}_per_tp_accuracy.png')\n",
    "        plot_score_layers(list(accuracy_dict.keys()), range(model.config.num_hidden_layers+1), f1_dict, 'f1', 'F1(macro)', f'{eval_target} classification', f'/home/s2420422/metano2/result/eval/{model_name}_{eval_target}_per_tp_f1.png')\n",
    "\n",
    "    '''\n",
    "    for model_name in model_name_list:\n",
    "        path_classification_report = f'/home/s2420422/metano2/result/eval/{model_name}_{eval_target}_classification_report.txt'\n",
    "        with open(path_classification_report, mode='w') as f:\n",
    "            for i, l in enumerate(classification_report_dict[model_name]):\n",
    "                print(i, file=f)\n",
    "                print(l, file=f)\n",
    "                print('---', file=f)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0c093",
   "metadata": {},
   "source": [
    "### eval df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(dd['train'])\n",
    "df_test = pd.DataFrame(dd['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e711538",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train['related_property_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28215892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.query('related_property_name not in [\"geographic distribution\", \"habitat\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202767f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['related_property_name'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e008793",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'related_property_name'\n",
    "\n",
    "train_counts = df_train[col_name].value_counts()\n",
    "test_counts = df_test[col_name].value_counts()\n",
    "\n",
    "# 両データを統合（カテゴリが一致しない場合に備えて外部結合を使用）\n",
    "combined_counts = pd.DataFrame({\n",
    "    'Train': train_counts,\n",
    "    'Test': test_counts\n",
    "}).fillna(0).sort_values(by='Train', ascending=False) \n",
    "\n",
    "# 折れ線グラフの描画\n",
    "combined_counts.plot(kind='line', marker='o')\n",
    "plt.title('Counts of related_property_name in Train and Test')\n",
    "plt.xlabel('Property Name')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(title='Dataset')\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)  # 必要に応じて軸ラベルを調整\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13995cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'property_name'\n",
    "\n",
    "train_counts = df_train[col_name].value_counts()\n",
    "test_counts = df_test[col_name].value_counts()\n",
    "\n",
    "# 両データを統合（カテゴリが一致しない場合に備えて外部結合を使用）\n",
    "combined_counts = pd.DataFrame({\n",
    "    'Train': train_counts,\n",
    "    'Test': test_counts\n",
    "}).fillna(0).sort_values(by='Train', ascending=False) \n",
    "\n",
    "# 折れ線グラフの描画\n",
    "combined_counts.plot(kind='line')\n",
    "plt.title(f'Counts of {col_name} in Train and Test')\n",
    "plt.xlabel(f'{col_name}')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(title='Dataset')\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)  # 必要に応じて軸ラベルを調整\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93233e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['property_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854d768",
   "metadata": {},
   "source": [
    "## multi mete rep to one-meta-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_fix_head = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_4x3_for_fix_head_train\")['train'].train_test_split(test_size=0.2)\n",
    "dd_fix_middle = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_4x3_for_fix_middle_train\")['train'].train_test_split(test_size=0.2)\n",
    "dd_fix_tail = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_4x3_for_fix_tail_train\")['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_32_1B_list1 = [f'model_32_1B_4x3_fix_head', f'model_32_1B_4x3_fix_tail', f'model_32_1B_4x3_fix_middle']\n",
    "\n",
    "tokenizer_32_1B1 = AutoTokenizer.from_pretrained(f\"kenken6696/Llama-3.2-1B_4x3_fix_tail\", trust_remote_code=True)\n",
    "model_32_1B_fix_tail1 = AutoModelForCausalLM.from_pretrained(f\"kenken6696/Llama-3.2-1B_4x3_fix_tail\", trust_remote_code=True, device_map='auto') # force_download=True\n",
    "model_32_1B_fix_head1 = AutoModelForCausalLM.from_pretrained(f\"kenken6696/Llama-3.2-1B_4x3_fix_head\", trust_remote_code=True, device_map='auto')\n",
    "model_32_1B_fix_middle1 = AutoModelForCausalLM.from_pretrained(f\"kenken6696/Llama-3.2-1B_4x3_fix_middle\", trust_remote_code=True, device_map='auto')\n",
    "\n",
    "hidden_state_ans_per_layer_dict_32_1B_fix_head1 = inference_and_collect_hidden_states_test(model_32_1B_fix_head1, tokenizer_32_1B1, dd_fix_head['train'],  len(dd_fix_head['train']),)\n",
    "hidden_state_ans_per_layer_dict_32_1B_fix_tail1 = inference_and_collect_hidden_states_test(model_32_1B_fix_tail1, tokenizer_32_1B1, dd_fix_tail['train'],  len(dd_fix_tail['train']),)\n",
    "hidden_state_ans_per_layer_dict_32_1B_fix_middle1 = inference_and_collect_hidden_states_test(model_32_1B_fix_middle1, tokenizer_32_1B1, dd_fix_middle['train'],  len(dd_fix_middle['train']),)\n",
    "\n",
    "hidden_state_ans_per_layer_dict_32_1Blist1 = \\\n",
    "    [hidden_state_ans_per_layer_dict_32_1B_fix_head1, hidden_state_ans_per_layer_dict_32_1B_fix_tail1, hidden_state_ans_per_layer_dict_32_1B_fix_middle1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_fix_3b = load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_4x3_for_fix_{meta_position}_train\")['train'].train_test_split(test_size=0.2)\n",
    "model_name_32_3B = f'model_32_3B_4x3_fix_{meta_position}'\n",
    "\n",
    "tokenizer_32_3B = AutoTokenizer.from_pretrained(f\"kenken6696/Llama-3.2-3B_4x3_fix_{meta_position}\", trust_remote_code=True)\n",
    "model_32_3B= AutoModelForCausalLM.from_pretrained(f\"kenken6696/Llama-3.2-3B_4x3_fix_{meta_position}\", trust_remote_code=True, device_map='auto')\n",
    "\n",
    "hidden_state_ans_per_layer_dict_32_3B_fix = inference_and_collect_hidden_states_test(model_32_3B, tokenizer_32_3B, dd_fix_3b['train'],  len(dd_fix_3b['train']),)\n",
    "\n",
    "import pickle\n",
    "with open(f'{model_name_32_3B}_hidden_state_ans_per_layer_dict.pkl', 'wb') as f:\n",
    "    pickle.dump({model_name_32_3B:hidden_state_ans_per_layer_dict_32_3B_fix}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba71a8",
   "metadata": {},
   "source": [
    "## mix model position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df145f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter and datasets\n",
    "base_model_names = ['Llama-3.2-1B', 'Llama-3.2-3B']\n",
    "base_model_name = base_model_names[0]\n",
    "meta_reps = [['known', 'unknown'], ['funny', 'boring'], ['biased', 'unbiased']]\n",
    "meta_rep_known, meta_rep_unknown = meta_reps[0]\n",
    "#-----------------------\n",
    "model_name = f'{base_model_name}_3_mix_position_{meta_rep_known}_{meta_rep_unknown}'\n",
    "dd= load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_3_mix_position_{meta_rep_known}_{meta_rep_unknown}_train\")['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"kenken6696/{model_name}\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"kenken6696/{model_name}\", trust_remote_code=True, device_map='auto') # force_download=True\n",
    "\n",
    "df = make_df_with_hidden_states_of_sentence(model, tokenizer, pd.DataFrame(dd['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f692fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name_list = ['meta_none', 'meta-tag']\n",
    "color_list = ['red','skyblue']\n",
    "accuracy_list_dict= defaultdict(list)\n",
    "\n",
    "accuracy_list_dict[score_name_list[0]], _, _,_=  \\\n",
    "    apply_probe_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, eval_target='meta_tag_none')\n",
    "accuracy_list_dict[score_name_list[1]], _, _,_=  \\\n",
    "    apply_probe_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, eval_target='meta_tag')\n",
    "\n",
    "plot_score_layers(score_name_list=score_name_list, layer_list=range(model.config.num_hidden_layers+1), score_dict=accuracy_list_dict, \\\n",
    "                  metrics_name='accuracy', y_label='Accuracy', title=f'{base_model_name} mix {meta_rep_known}, {meta_rep_unknown} classification', output_path=f'/home/s2420422/metano2/result/eval/{model_name}_basic_accuracy.png', \\\n",
    "                    chance_rate=0.5,color_l=color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5214bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name_list = ['head,middle-tail', 'tail-tail', 'middle,tail-head', 'head-head', 'tail,head-middle', 'middle-middle']\n",
    "color_list = ['skyblue', 'steelblue','red','darkred','lightgreen','olive']\n",
    "accuracy_list_dict= defaultdict(list)\n",
    "\n",
    "accuracy_list_dict[score_name_list[0]],_=  \\\n",
    "    apply_probe_cv_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, meta_position_flag=True, train_target_meta_positions=['head', 'middle'], test_target_meta_positions=['tail'])\n",
    "accuracy_list_dict[score_name_list[1]],_=  \\\n",
    "    apply_probe_cv_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, meta_position_flag=True, train_target_meta_position_same_as_test=True, test_target_meta_positions=['tail'])\n",
    "accuracy_list_dict[score_name_list[2]],_=  \\\n",
    "    apply_probe_cv_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, meta_position_flag=True, train_target_meta_positions=['tail', 'middle'], test_target_meta_positions=['head'])\n",
    "accuracy_list_dict[score_name_list[3]],_=  \\\n",
    "    apply_probe_cv_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, meta_position_flag=True, train_target_meta_position_same_as_test=True, test_target_meta_positions=['head'])\n",
    "accuracy_list_dict[score_name_list[4]],_=  \\\n",
    "    apply_probe_cv_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, meta_position_flag=True, train_target_meta_positions=['head', 'tail'], test_target_meta_positions=['middle'])\n",
    "accuracy_list_dict[score_name_list[5]],_=  \\\n",
    "    apply_probe_cv_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, meta_position_flag=True, train_target_meta_position_same_as_test=True, test_target_meta_positions=['middle'])\n",
    "\n",
    "plot_score_layers(score_name_list=score_name_list, layer_list=range(model.config.num_hidden_layers+1), score_dict=accuracy_list_dict, metrics_name='accuracy', y_label='Accuracy', \\\n",
    "                  title=f'{base_model_name} meta-tag classification by meta-position', output_path=f'/home/s2420422/metano2/result/eval/{model_name}_accuracy_by_position.png', chance_rate=0.5,color_l=color_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20df13",
   "metadata": {},
   "source": [
    "## mix model position 3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter and datasets\n",
    "base_model_names = ['Llama-3.2-1B', 'Llama-3.2-3B']\n",
    "base_model_name = base_model_names[1]\n",
    "#-----------------------\n",
    "model_name = f'{base_model_name}_3x3_mix_position'\n",
    "dd= load_dataset(f\"kenken6696/ALCUNA_meta_affirmative_3x3_for_mix_position_train\")['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"kenken6696/{model_name}\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"kenken6696/{model_name}\", trust_remote_code=True, device_map='auto') # force_download=True\n",
    "\n",
    "df = make_df_with_hidden_states_of_sentence(model, tokenizer, pd.DataFrame(dd['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name_list = ['meta_none', 'meta-tag']\n",
    "color_list = ['red','skyblue']\n",
    "accuracy_list_dict= defaultdict(list)\n",
    "\n",
    "accuracy_list_dict[score_name_list[0]], _, _,_=  \\\n",
    "    apply_probe_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, eval_target='meta_tag_none')\n",
    "accuracy_list_dict[score_name_list[1]], _, _,_=  \\\n",
    "    apply_probe_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, eval_target='meta_tag')\n",
    "\n",
    "plot_score_layers(score_name_list=score_name_list, layer_list=range(model.config.num_hidden_layers+1), score_dict=accuracy_list_dict, \\\n",
    "                  metrics_name='accuracy', y_label='Accuracy', title=f'{base_model_name} classification', output_path=f'/home/s2420422/metano2/result/eval/{model_name}_meta_position_accuracy.png', \\\n",
    "                    chance_rate=0.5,color_l=color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_reps_known=['known', 'famous', 'understood']\n",
    "meta_reps_unknown=['unknown', 'unrecognized', 'unfamiliar']\n",
    "meta_reps_known_remain = copy.copy(meta_reps_known)\n",
    "meta_reps_unknown_remain = copy.copy(meta_reps_unknown)\n",
    "score_name_list = list()\n",
    "accuracy_list_dict= defaultdict(list)\n",
    "\n",
    "for meta_rep_known in meta_reps_known:\n",
    "    meta_reps_known_remain.remove(meta_rep_known)\n",
    "    for meta_rep_unknown in meta_reps_unknown:\n",
    "        meta_reps_unknown_remain.remove(meta_rep_unknown)\n",
    "\n",
    "        print(f'{meta_reps_known_remain},{meta_reps_unknown_remain}-{meta_rep_known},{meta_rep_unknown}')\n",
    "\n",
    "        score_name_list.append(f'{meta_reps_known_remain},{meta_reps_unknown_remain}-{meta_rep_known},{meta_rep_unknown}')\n",
    "        accuracy_list_dict[(f'{meta_reps_known_remain},{meta_reps_unknown_remain}-{meta_rep_known},{meta_rep_unknown}')], _ = \\\n",
    "            apply_probe_cv_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, meta_reps_flag=True, \\\n",
    "                                    train_target_meta_reps_dict={\"known\":meta_reps_known_remain, \"unknown\":meta_reps_unknown_remain}, \\\n",
    "                                    test_target_meta_reps_dict={\"known\":meta_rep_known, \"unknown\":meta_rep_unknown})\n",
    "        \n",
    "        score_name_list.append(f'{meta_rep_known},{meta_rep_unknown}-{meta_rep_known},{meta_rep_unknown}')\n",
    "        accuracy_list_dict[(f'{meta_rep_known},{meta_rep_unknown}-{meta_rep_known},{meta_rep_unknown}')], _ = \\\n",
    "            apply_probe_cv_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, meta_reps_flag=True, \\\n",
    "                                    train_target_meta_reps_same_as_test=True,\\\n",
    "                                    test_target_meta_reps_dict={\"known\":meta_rep_known, \"unknown\":meta_rep_unknown})       \n",
    "        meta_reps_unknown_remain = copy.copy(meta_reps_unknown)\n",
    "    meta_reps_known_remain = copy.copy(meta_reps_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_layers(score_name_list=score_name_list, layer_list=range(model.config.num_hidden_layers+1), score_dict=accuracy_list_dict, metrics_name='accuracy', y_label='Accuracy', \\\n",
    "                  title=f'{base_model_name} meta-tag classification by meta-group', output_path=f'/home/s2420422/metano2/result/eval/{model_name}_accuracy_by_rep.png', chance_rate=0.5,\\\n",
    "                    color_l = ['red', 'blue', 'green', 'orange', 'purple', 'brown','pink', 'gray', 'olive', 'cyan', 'magenta', 'lime','teal', 'navy', 'gold', 'darkred', 'darkgreen', 'darkblue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e09e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name_list = ['var1_var_2-var3', 'var3-var3']\n",
    "var1_2_var3_accuracy_l, var3_var3_accuracy_l = list(), list()\n",
    "for i in range(model.config.num_hidden_layers+1):\n",
    "    var1_2_var3_temp_l, var3_var3_temp_l = list(), list()\n",
    "    for score_name in list(accuracy_list_dict.keys()):\n",
    "        if score_name.find('[') == -1:\n",
    "            var1_2_var3_temp_l.append(accuracy_list_dict[score_name]['accuracy'])\n",
    "        else:\n",
    "            var3_var3_temp_l.append(accuracy_list_dict[score_name]['accuracy'])\n",
    "    print(var1_2_var3_temp_l, var3_var3_temp_l)\n",
    "    q1, q2, q3 = statistics.quantiles(var1_2_var3_temp_l)\n",
    "    var1_2_var3_accuracy_l.append({'accuracy': round(statistics.mean(var1_2_var3_temp_l)), 'q1': })\n",
    "    var3_var3_accuracy_l.append({'accuracy': round(statistics.mean(var3_var3_temp_l))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_reps_known=['known', 'famous', 'understood']\n",
    "meta_reps_unknown=['unknown', 'unrecognized', 'unfamiliar']\n",
    "meta_reps_known_remain = copy.copy(meta_reps_known)\n",
    "meta_reps_unknown_remain = copy.copy(meta_reps_unknown)\n",
    "score_name_list = list()\n",
    "accuracy_list_dict= defaultdict(list)\n",
    "\n",
    "for meta_rep_known in meta_reps_known:\n",
    "    meta_reps_known_remain.remove(meta_rep_known)\n",
    "    for meta_rep_unknown in meta_reps_unknown:\n",
    "        meta_reps_unknown_remain.remove(meta_rep_unknown)\n",
    "\n",
    "        print(f'{meta_reps_known_remain},{meta_reps_unknown_remain}-{meta_rep_known},{meta_rep_unknown}')\n",
    "\n",
    "        score_name_list.append(f'{meta_reps_known_remain},{meta_reps_unknown_remain}-{meta_rep_known},{meta_rep_unknown}')\n",
    "        accuracy_list_dict[(f'{meta_reps_known_remain},{meta_reps_unknown_remain}-{meta_rep_known},{meta_rep_unknown}')], _ = \\\n",
    "            apply_probe_cv_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, meta_reps_flag=True, target_meta_position=\"tail\",\\\n",
    "                                    train_target_meta_reps_dict={\"known\":meta_reps_known_remain, \"unknown\":meta_reps_unknown_remain}, \\\n",
    "                                    test_target_meta_reps_dict={\"known\":meta_rep_known, \"unknown\":meta_rep_unknown})\n",
    "        \n",
    "        score_name_list.append(f'{meta_rep_known},{meta_rep_unknown}-{meta_rep_known},{meta_rep_unknown}')\n",
    "        accuracy_list_dict[(f'{meta_rep_known},{meta_rep_unknown}-{meta_rep_known},{meta_rep_unknown}')], _ = \\\n",
    "            apply_probe_cv_to_hidden_states(layer_num=model.config.num_hidden_layers+1, original_df_with_hs=df, meta_reps_flag=True, \\\n",
    "                                    train_target_meta_reps_same_as_test=True, target_meta_position=\"tail\",\\\n",
    "                                    test_target_meta_reps_dict={\"known\":meta_rep_known, \"unknown\":meta_rep_unknown})       \n",
    "        meta_reps_unknown_remain = copy.copy(meta_reps_unknown)\n",
    "    meta_reps_known_remain = copy.copy(meta_reps_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59129bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_layers(score_name_list=score_name_list, layer_list=range(model.config.num_hidden_layers+1), score_dict=accuracy_list_dict, metrics_name='accuracy', y_label='Accuracy', \\\n",
    "                  title=f'{base_model_name} meta-tag classification by meta-group-head', output_path=f'/home/s2420422/metano2/result/eval/{model_name}_accuracy_by_position.png', chance_rate=0.5,\\\n",
    "                    color_l = ['red', 'blue', 'green', 'orange', 'purple', 'brown','pink', 'gray', 'olive', 'cyan', 'magenta', 'lime','teal', 'navy', 'gold', 'darkred', 'darkgreen', 'darkblue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03680545",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd176211",
   "metadata": {},
   "source": [
    "### prompt accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac91904",
   "metadata": {},
   "source": [
    "#### 知識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21330208",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl = [dd_fix_head['test'], dd_fix_middle['test'], dd_fix_tail['test']]\n",
    "ds_name = ['fix_head', 'fix_middle', 'fix_tail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_alcuna_4 = \"### Instruction: I will ask a multiple-choice question. Please respond with the number of the option you believe is correct.\\n\\n\\\n",
    "### Question:'What is the geographic distribution of Red Fox?\\n0. Africa\\n\\n1. Antarctica\\n\\n2. Sahara Desert\\n\\n3. Himalayas'\\n### Answer:0\\n\\\n",
    "### Question:'Which continent is home to the Amazon Rainforest?\\n0. Africa\\n\\n1. Asia\\n\\n2. South America\\n\\n3. Australia'\\n### Answer:2\\n\\\n",
    "### Question:'What is the primary habitat of polar bears?\\n0. Tropical Rainforest\\n\\n1. Arctic Region\\n\\n2. Grasslands\\n\\n3. Desert'\\n### Answer:1\\n\\\n",
    "### Question:'Where are the Galapagos Islands located?\\n0. Arctic Ocean\\n\\n1. Atlantic Ocean\\n\\n2. Indian Ocean\\n\\n3. Pacific Ocean'\\n### Answer:3\\n\\\n",
    "### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_alcuna_4_noinst = \"### Question:'Where are the Galapagos Islands located?\\n0. Arctic Ocean\\n\\n1. Atlantic Ocean\\n\\n2. Indian Ocean\\n\\n3. Pacific Ocean'\\n### Answer:3\\n\\\n",
    "### Question:'What is the geographic distribution of Red Fox?\\n0. Africa\\n\\n1. Antarctica\\n\\n2. Sahara Desert\\n\\n3. Himalayas'\\n### Answer:0\\n\\\n",
    "### Question:'Which continent is home to the Amazon Rainforest?\\n0. Africa\\n\\n1. Asia\\n\\n2. South America\\n\\n3. Australia'\\n### Answer:2\\n\\\n",
    "### Question:'What is the primary habitat of polar bears?\\n0. Tropical Rainforest\\n\\n1. Arctic Region\\n\\n2. Grasslands\\n\\n3. Desert'\\n### Answer:1\\n\\\n",
    "### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_alcuna_1 = \"### Instruction: I will ask a multiple-choice question. Please respond with only the number of the option you believe is correct. Before the question, I will provide some examples to demonstrate how to answer.\\n\\n\\\n",
    "### Question:'What is the geographic distribution of Red Fox?\\n0. Africa\\n\\n1. Antarctica\\n\\n2. Sahara Desert\\n\\n3. Himalayas'\\n### Answer:0\\n\\\n",
    "### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_alcuna_1_noinst = \"### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_alcuna_0 = \"### Instruction: I will ask a multiple-choice question. Please respond with only the number of the option you believe is correct. Before the question, I will provide some examples to demonstrate how to answer.\\n\\n\\\n",
    "### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_alcuna_0_noinst = \"### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_name_l = ['prompt_alcuna_4', 'prompt_alcuna_4_noinst','prompt_alcuna_1','prompt_alcuna_1_noinst','prompt_alcuna_0','prompt_alcuna_0_noinst']\n",
    "prompt_l = [prompt_alcuna_4, prompt_alcuna_4_noinst, prompt_alcuna_1, prompt_alcuna_1_noinst, prompt_alcuna_0, prompt_alcuna_0_noinst]\n",
    "\n",
    "def make_alcuna_answer(question, prompt=prompt_alcuna_4 ,model=\"gpt-4o-mini\") -> str:\n",
    "    alcuna_answer = get_res(model, prompt.format(question=question))\n",
    "    return alcuna_answer\n",
    "\n",
    "def extract_answer(output, sample_num):\n",
    "    for _ in range(sample_num): # magic number for prompt_alcuna\n",
    "        sample_ans_skip = output.find('### Answer:')+12\n",
    "        output = output[sample_ans_skip:]\n",
    "\n",
    "    try:\n",
    "        acutual_ans_start = output.find('### Answer:')\n",
    "        ans =  output[acutual_ans_start:][11]\n",
    "        if ans not in ['0','1','2','3']:\n",
    "            ans = '-1'\n",
    "    except Exception:\n",
    "        ans = '-2'\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26734030",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_meta_ = \"### Instruction: I will ask a multiple-choice question. Please respond with the number of the option you believe is correct.\\n\\n\\\n",
    "### Question:'What is the geographic distribution of Red Fox?\\n0. Africa\\n\\n1. Antarctica\\n\\n2. Sahara Desert\\n\\n3. Himalayas'\\n### Answer:0\\n\\\n",
    "### Question:'Which continent is home to the Amazon Rainforest?\\n0. Africa\\n\\n1. Asia\\n\\n2. South America\\n\\n3. Australia'\\n### Answer:2\\n\\\n",
    "### Question:'What is the primary habitat of polar bears?\\n0. Tropical Rainforest\\n\\n1. Arctic Region\\n\\n2. Grasslands\\n\\n3. Desert'\\n### Answer:1\\n\\\n",
    "### Question:'Where are the Galapagos Islands located?\\n0. Arctic Ocean\\n\\n1. Atlantic Ocean\\n\\n2. Indian Ocean\\n\\n3. Pacific Ocean'\\n### Answer:3\\n\\\n",
    "### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_alcuna_4_noinst = \"### Question:'Where are the Galapagos Islands located?\\n0. Arctic Ocean\\n\\n1. Atlantic Ocean\\n\\n2. Indian Ocean\\n\\n3. Pacific Ocean'\\n### Answer:3\\n\\\n",
    "### Question:'What is the geographic distribution of Red Fox?\\n0. Africa\\n\\n1. Antarctica\\n\\n2. Sahara Desert\\n\\n3. Himalayas'\\n### Answer:0\\n\\\n",
    "### Question:'Which continent is home to the Amazon Rainforest?\\n0. Africa\\n\\n1. Asia\\n\\n2. South America\\n\\n3. Australia'\\n### Answer:2\\n\\\n",
    "### Question:'What is the primary habitat of polar bears?\\n0. Tropical Rainforest\\n\\n1. Arctic Region\\n\\n2. Grasslands\\n\\n3. Desert'\\n### Answer:1\\n\\\n",
    "### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_alcuna_1 = \"### Instruction: I will ask a multiple-choice question. Please respond with only the number of the option you believe is correct. Before the question, I will provide some examples to demonstrate how to answer.\\n\\n\\\n",
    "### Question:'What is the geographic distribution of Red Fox?\\n0. Africa\\n\\n1. Antarctica\\n\\n2. Sahara Desert\\n\\n3. Himalayas'\\n### Answer:0\\n\\\n",
    "### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_alcuna_1_noinst = \"### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_alcuna_0 = \"### Instruction: I will ask a multiple-choice question. Please respond with only the number of the option you believe is correct. Before the question, I will provide some examples to demonstrate how to answer.\\n\\n\\\n",
    "### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_alcuna_0_noinst = \"### Question:'{question}'\\n### Answer:\"\n",
    "\n",
    "prompt_name_l = ['prompt_alcuna_4', 'prompt_alcuna_4_noinst','prompt_alcuna_1','prompt_alcuna_1_noinst','prompt_alcuna_0','prompt_alcuna_0_noinst']\n",
    "prompt_l = [prompt_alcuna_4, prompt_alcuna_4_noinst, prompt_alcuna_1, prompt_alcuna_1_noinst, prompt_alcuna_0, prompt_alcuna_0_noinst]\n",
    "\n",
    "def make_alcuna_answer(question, prompt=prompt_alcuna_4 ,model=\"gpt-4o-mini\") -> str:\n",
    "    alcuna_answer = get_res(model, prompt.format(question=question))\n",
    "    return alcuna_answer\n",
    "\n",
    "def extract_answer(output, sample_num):\n",
    "    for _ in range(sample_num): # magic number for prompt_alcuna\n",
    "        sample_ans_skip = output.find('### Answer:')+12\n",
    "        output = output[sample_ans_skip:]\n",
    "\n",
    "    try:\n",
    "        acutual_ans_start = output.find('### Answer:')\n",
    "        ans =  output[acutual_ans_start:][11]\n",
    "        if ans not in ['0','1','2','3']:\n",
    "            ans = '-1'\n",
    "    except Exception:\n",
    "        ans = '-2'\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate([model_32_1B_fix_head, model_32_1B_fix_middle, model_32_1B_fix_tail]):\n",
    "    tokenizer = tokenizer_32_1B\n",
    "    temp_ds = dsl[i] # dslの順序は、head/middle/tail、あとできれいにする\n",
    "\n",
    "    for p, p_name in zip(prompt_l, prompt_name_l):\n",
    "        ans_l = list()\n",
    "        p_loop_num = int(re.sub(r\"\\D\", \"\", p_name))\n",
    "        \n",
    "        with tqdm(temp_ds) as ds_epoch:\n",
    "            for batch in ds_epoch:\n",
    "                prmpt = p.format(question=batch['question'])\n",
    "                input = tokenizer(prmpt, return_tensors=\"pt\").to(device)\n",
    "                ans_sentences = tokenizer.decode(model.generate(**input, max_new_tokens=len(input.input_ids[0])*0.5)[0])\n",
    "                ans_l.append(extract_answer(ans_sentences, p_loop_num))\n",
    "    \n",
    "        temp_ds = temp_ds.add_column(name=f\"ans_{p_name}\", column=ans_l)\n",
    "    \n",
    "    dsl[i] = temp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02439ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds, name in zip(dsl, ds_name):\n",
    "    ds.save_to_disk(f\"/home/s2420422/metano2/result/datasets/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "results = defaultdict(list)\n",
    "\n",
    "for ds, name in zip(dsl, ds_name):# = [dd_fix_head['test'], dd_fix_middle['test'], dd_fix_tail['test']]\n",
    "\n",
    "    for p_name in prompt_name_l:\n",
    "        result = []\n",
    "        result.append(accuracy_metric.compute(references=ds['answer'], predictions=ds[f'ans_{p_name}']))\n",
    "        result.append(f1_metric.compute(references=ds['answer'], predictions=ds[f'ans_{p_name}'], average='macro'))\n",
    "        results[name] = result\n",
    "\n",
    "    \n",
    "        # show\n",
    "        df = pd.DataFrame(ds)\n",
    "        print(name)\n",
    "        print(results[name])\n",
    "        print(df[f'ans_{p_name}'].value_counts())\n",
    "        print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afe043",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in dsl:\n",
    "    df = pd.DataFrame(ds)\n",
    "    print(df.answer.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds, name in zip(dsl, ds_name):\n",
    "    for p_name in prompt_name_l:\n",
    "        df = pd.DataFrame(ds)\n",
    "        print(name)\n",
    "        print(results[name])\n",
    "        print(df[f'ans_{p_name}'].value_counts())\n",
    "        print('--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794caf9",
   "metadata": {},
   "source": [
    "### prover 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8fd003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old\n",
    "\n",
    "def train_test_split_from_hidden_state_ans_per_layer_dict(hidden_state_ans_per_layer_dict, layer_num, token_position=-1, ans_for3=True, test_size=0.25, random_state=RANDOM_SEED, mix_position=False):\n",
    "    \n",
    "    hidden_state_ans_per_layer = hidden_state_ans_per_layer_dict[layer_num]\n",
    "    hidden_states_list, ans_list = list(), list()\n",
    "    if ans_for3 == True:\n",
    "        for dic in hidden_state_ans_per_layer:\n",
    "            hidden_states_list.append(dic['hidden_states'][token_position]) # input_token最終 (emb_size)\n",
    "            ans_list.append(dic['meta_tag'])\n",
    "    else:\n",
    "        for dic in hidden_state_ans_per_layer:\n",
    "            if dic['meta_tag'] != 'none':\n",
    "                # known/unknownの性能のみを評価する\n",
    "                hidden_states_list.append(dic['hidden_states'][token_position]) # input_token最終 (emb_size)\n",
    "                ans_list.append(dic['meta_tag'])\n",
    "            \n",
    "            \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        hidden_states_list, \n",
    "        ans_list, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def eval_probe_crosstest(model_name, main_train_test, cross_train_test_list, cross_model_name_list):\n",
    "    X_train, X_test, y_train, y_test = main_train_test\n",
    "    accuracy_list = defaultdict(list) # {model_name_1:{layer: }}\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = main_train_test\n",
    "    #print(f'train:{y_train}, X_train:{X_train}')\n",
    "    \n",
    "    # Scale data based on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit a Logistic Regression model\n",
    "    clf = LogisticRegression(max_iter=200)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Calculate R^2 score and accuracyon test data\n",
    "    #r2_test = 1#r2_score(y_test, clf.predict(X_test_scaled))\n",
    "    #r2_score_list[model_name].append(r2_test)\n",
    "    accuracy_test = accuracy_score(y_test, clf.predict(X_test_scaled))\n",
    "    #accuracy_test = classification_report(y_test, clf.predict(X_test_scaled))\n",
    "    accuracy_list[model_name].append(accuracy_test)\n",
    "\n",
    "    for cross_train_test, cross_model_name in zip(cross_train_test_list, cross_model_name_list):\n",
    "        _, cX_test, _, cy_test = cross_train_test\n",
    "        cX_test_scaled = scaler.transform(cX_test)\n",
    "        #accuracy_test = classification_report(cy_test, clf.predict(cX_test_scaled))\n",
    "        accuracy_test = accuracy_score(cy_test, clf.predict(cX_test_scaled))\n",
    "        accuracy_list[cross_model_name].append(accuracy_test)\n",
    "\n",
    "\n",
    "    # Log important metrics\n",
    "    #logger.info(f\"token_position:{token_position},Layer: {layer}, Test Accuracy: {accuracy_test}\")\n",
    "    #logger.info(f\"Model: {model}, Layer: {layer}, Test R^2: {r2_test}, Test Accuracy: {accuracy_test}\")\n",
    "                \n",
    "    return accuracy_list\n",
    "\n",
    "def apply_probe_to_hidden_states_test(\n",
    "    model_name_list, layer_num, hidden_state_ans_per_layer_dict_list, target_flag=False, train_target_meta_reps=None, test_target_meta_reps=None, token_position=-1\n",
    "):\n",
    "    accuracy_list_for3, accuracy_list_for2, accuracy_list_for_meta, accuracy_list_by_rep = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list) # {model_name_1:{layer: }}\n",
    "    f1_list_for3, f1_list_for2, f1_list_for_meta, f1_list_by_rep = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list) # {model_name_1:{layer: }}\n",
    "    classification_report_list_for3, classification_report_list_for2, classification_report_list_for_meta, classification_report_list_by_rep = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    for model_name, hidden_state_ans_per_layer_dict in zip(model_name_list, hidden_state_ans_per_layer_dict_list):\n",
    "\n",
    "        for layer in range(layer_num):\n",
    "            hidden_states_list_for3, ans_list_for3, meta_rep_list_for3 = list(), list(), list()\n",
    "            hidden_states_list_for2, ans_list_for2, meta_rep_list_for2 = list(), list(), list()\n",
    "            hidden_states_list_for_meta, ans_list_for_meta, meta_rep_list_for_meta = list(), list(), list()\n",
    "            hidden_state_ans_per_layer = hidden_state_ans_per_layer_dict[layer]\n",
    "            for dic in hidden_state_ans_per_layer:\n",
    "\n",
    "                hidden_states_list_for3.append(dic['hidden_states'][token_position]) # input_token最終 (emb_size)\n",
    "                ans_list_for3.append(dic['meta_tag'])\n",
    "                meta_rep_list_for3.append(dic['meta_rep'])\n",
    "                if dic['meta_tag'] != 'none':\n",
    "                    # メタ認知表現ありのみの性能のみを評価する\n",
    "                    hidden_states_list_for_meta.append(dic['hidden_states'][token_position]) # input_token最終 (emb_size)\n",
    "                    ans_list_for_meta.append(dic['meta_tag'])\n",
    "                    meta_rep_list_for_meta.append(dic['meta_rep'])\n",
    "                    if dic['meta_tag'] in ['known', 'unknown']:\n",
    "                        hidden_states_list_for2.append(dic['hidden_states'][token_position]) # input_token最終 (emb_size)\n",
    "                        ans_list_for2.append(dic['meta_tag'])\n",
    "                        meta_rep_list_for2.append(dic['meta_rep'])\n",
    "\n",
    "            for hidden_states_list, meta_rep_list, accuracy_list, f1_list, classification_report_list in zip ([hidden_states_list_for3] ,[meta_rep_list_for3], [accuracy_list_by_rep], [f1_list_by_rep], [classification_report_list_by_rep]):\n",
    "                meta_rep_list = [str(x) for x in meta_rep_list]\n",
    "\n",
    "                X_train_rep, X_test_rep, y_train_rep, y_test_rep = train_test_split(\n",
    "                    hidden_states_list, \n",
    "                    meta_rep_list, \n",
    "                    test_size=0.25, \n",
    "                    random_state=RANDOM_SEED\n",
    "                    )\n",
    "                \n",
    "                # Scale data based on training data\n",
    "                scaler = StandardScaler()\n",
    "                X_train_rep_scaled = scaler.fit_transform(X_train_rep)\n",
    "                X_test_rep_scaled = scaler.transform(X_test_rep)\n",
    "                \n",
    "                # Fit a Logistic Regression model\n",
    "                clf = LogisticRegression(max_iter=200)\n",
    "                clf.fit(X_train_rep_scaled, y_train_rep)\n",
    "                \n",
    "                # Calculate R^2 score and accuracyon test data\n",
    "                #r2_test = 1#r2_score(y_test, clf.predict(X_test_scaled))\n",
    "                #r2_score_list[model_name].append(r2_test)\n",
    "                #accuracy_test = accuracy_score(y_test, clf.predict(X_test_scaled))\n",
    "                #accuracy_test = classification_report(y_test, clf.predict(X_test_scaled))\n",
    "\n",
    "                le_rep = LabelEncoder()\n",
    "                unique_meta_rep = set(meta_rep_list)\n",
    "                unique_meta_rep.discard(None)\n",
    "                unique_meta_rep = list(unique_meta_rep)\n",
    "                le_rep.fit(unique_meta_rep)\n",
    "                print(f'unique_meta_rep size: {len(unique_meta_rep)}')\n",
    "\n",
    "                accuracy_test = accuracy_metric.compute(references=le_rep.transform(y_test_rep), predictions=le_rep.transform(clf.predict(X_test_rep_scaled)))\n",
    "                f1_test = f1_metric.compute(references=le_rep.transform(y_test_rep), predictions=le_rep.transform(clf.predict(X_test_rep_scaled)), average='macro')\n",
    "\n",
    "                classification_report_list[model_name].append(classification_report(y_test_rep, clf.predict(X_test_rep_scaled)))\n",
    "\n",
    "                accuracy_list[model_name].append(accuracy_test)\n",
    "                f1_list[model_name].append(f1_test)\n",
    "\n",
    "            \n",
    "            for hidden_states_list, ans_list, meta_rep_list, accuracy_list, f1_list, classification_report_list in zip ([hidden_states_list_for3, hidden_states_list_for2, hidden_states_list_for_meta], [ans_list_for3, ans_list_for2, ans_list_for_meta], [meta_rep_list_for3, meta_rep_list_for2, meta_rep_list_for_meta],[accuracy_list_for3, accuracy_list_for2, accuracy_list_for_meta], [f1_list_for3, f1_list_for2, f1_list_for_meta], [classification_report_list_for3, classification_report_list_for2, classification_report_list_for_meta]):\n",
    "            \n",
    "                # Split data into training and testing sets\n",
    "                if target_flag == False:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        hidden_states_list, \n",
    "                        ans_list, \n",
    "                        test_size=0.25, \n",
    "                        random_state=RANDOM_SEED\n",
    "                    )\n",
    "                else:\n",
    "                # train_target, test_targetは排反想定\n",
    "                    X_train, X_test, y_train, y_test = list(), list(), list(), list()\n",
    "\n",
    "                    for i, rep in enumerate(meta_rep_list):\n",
    "                        if rep in train_target_meta_reps:\n",
    "                            X_train.append(hidden_states_list[i])\n",
    "                            y_train.append(ans_list[i])\n",
    "                        elif rep in test_target_meta_reps:\n",
    "                            X_test.append(hidden_states_list[i])\n",
    "                            y_test.append(ans_list[i])\n",
    "                \n",
    "                # Scale data based on training data\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Fit a Logistic Regression model\n",
    "                clf = LogisticRegression(max_iter=200)\n",
    "                clf.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                # Calculate R^2 score and accuracyon test data\n",
    "                #r2_test = 1#r2_score(y_test, clf.predict(X_test_scaled))\n",
    "                #r2_score_list[model_name].append(r2_test)\n",
    "                #accuracy_test = accuracy_score(y_test, clf.predict(X_test_scaled))\n",
    "                #accuracy_test = classification_report(y_test, clf.predict(X_test_scaled))\n",
    " \n",
    "                le = LabelEncoder()\n",
    "                le.fit(['known', 'unknown', 'none', 'others'])\n",
    "\n",
    "                accuracy_test = accuracy_metric.compute(references=le.transform(y_test), predictions=le.transform(clf.predict(X_test_scaled)))\n",
    "                f1_test = f1_metric.compute(references=le.transform(y_test), predictions=le.transform(clf.predict(X_test_scaled)), average='macro')\n",
    "\n",
    "                accuracy_list[model_name].append(accuracy_test)\n",
    "                f1_list[model_name].append(f1_test)\n",
    "\n",
    "                classification_report_list[model_name].append(classification_report(y_test, clf.predict(X_test_scaled)))\n",
    "                # Log important metrics\n",
    "                #logger.info(f\"token_position:{token_position},Layer: {layer}, Test Accuracy: {accuracy_test}\")\n",
    "                #logger.info(f\"Model: {model}, Layer: {layer}, Test R^2: {r2_test}, Test Accuracy: {accuracy_test}\")\n",
    "                \n",
    "    return accuracy_list_for3, accuracy_list_for2, accuracy_list_for_meta, accuracy_list_by_rep, f1_list_for3, f1_list_for2, f1_list_for_meta, f1_list_by_rep, classification_report_list_for3, classification_report_list_for2, classification_report_list_for_meta, classification_report_list_by_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_layers(score_name_list, layer_list, score_list, metrics_name, y_label, title, output_path):\n",
    "    # Plot score values across layers with distinct styles for clarity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    linestyle_l = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "    color_l = ['r','y','b','m']\n",
    "\n",
    "    for score_name in score_name_list:\n",
    "\n",
    "        plt.plot(\n",
    "            layer_list,\n",
    "            [dic[metrics_name] for dic in score_list[score_name]],\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            label=score_name\n",
    "        )\n",
    "\n",
    "    \n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    #plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "\n",
    "def plot_score_crosstest(layer_start, layer_end, accuracy_list_per_layer, y_label, title, output_path, chance_rate=1/3):\n",
    "    # Plot score values across layers with distinct styles for clarity\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    linestyle_l = [\"solid\", \"dashed\", \"dashdot\"]\n",
    "    color_l = ['r','y','b']\n",
    "\n",
    "    for layer in range(layer_start, layer_end+1):\n",
    "\n",
    "        score_dict = accuracy_list_per_layer[layer]\n",
    "\n",
    "        plt.plot(\n",
    "            list(score_dict.keys()),\n",
    "            [v[0] for v in score_dict.values()],\n",
    "            #linestyle=linestyle_l[i],\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            label=f'layer {layer}'\n",
    "        )\n",
    "\n",
    "    plt.axhline(y=chance_rate, color='gray', linestyle='dotted', linewidth=2, label='Chance Rate')\n",
    "    \n",
    "    plt.xlabel(\"hidden states\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    #plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    #plt.savefig(output_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f059e2",
   "metadata": {},
   "source": [
    "### not cross test 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name_32_3B_list = [f'model_32_3B_{meta_rep_known}_{meta_rep_unknown}_fix_head',f'model_32_3B_{meta_rep_known}_{meta_rep_unknown}_fix_middle',f'model_32_3B_{meta_rep_known}_{meta_rep_unknown}_fix_tail']\n",
    "model_name_32_3B_list = ['model_32_3B_4x3_fix_head','model_32_3B_4x3_fix_middle','model_32_3B_4x3_fix_tail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3つろーどしてつくる\n",
    "hidden_state_ans_per_layer_dict_32_3Blist = []\n",
    "\n",
    "for model_name_32_3B in model_name_32_3B_list:\n",
    "    with open(f'{model_name_32_3B}_hidden_state_ans_per_layer_dict.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "        hidden_state_ans_per_layer_dict_32_3Blist.append(d[model_name_32_3B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_for3, accuracy_list_for2, accuracy_list_for_meta, accuracy_list_by_rep, f1_list_for3, f1_list_for2, f1_list_for_meta, f1_list_by_rep, classification_report_list_for3, classification_report_list_for2, classification_report_list_for_meta, classification_report_list_by_rep =  \\\n",
    "        apply_probe_to_hidden_states_test(model_name_list=model_name_32_3B_list, layer_num=model_32_3B.config.num_hidden_layers+1, hidden_state_ans_per_layer_dict_list=hidden_state_ans_per_layer_dict_32_3Blist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd239ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_layers(model_name_32_3B_list, range(model_32_3B.config.num_hidden_layers+1), accuracy_list_for2, 'accuracy', 'Accuracy', 'binary(known, unknown) classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_3B_list[0]}_accuracy_list_for2_32_3B.png')\n",
    "plot_score_layers(model_name_32_3B_list, range(model_32_3B.config.num_hidden_layers+1), accuracy_list_for3, 'accuracy',  'Accuracy', 'meta+none classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_3B_list[0]}_accuracy_list_for3_32_3B_.png')\n",
    "plot_score_layers(model_name_32_3B_list, range(model_32_3B.config.num_hidden_layers+1), f1_list_for3, 'f1',  'F1(macro)', 'meta+none classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_3B_list[0]}_f1_list_for3_32_3B.png')\n",
    "plot_score_layers(model_name_32_3B_list, range(model_32_3B.config.num_hidden_layers+1), accuracy_list_for_meta, 'accuracy',  'Accuracy', 'meta classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_3B_list[0]}_accuracy_list_for3_32_3B.png')\n",
    "plot_score_layers(model_name_32_3B_list, range(model_32_3B.config.num_hidden_layers+1), f1_list_for_meta, 'f1',  'F1(macro)', 'meta classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_3B_list[0]}_f1_list_for3_32_3B.png')\n",
    "plot_score_layers(model_name_32_3B_list, range(model_32_3B.config.num_hidden_layers+1), accuracy_list_by_rep, 'accuracy',  'Accuracy', 'meta-rep classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_3B_list[0]}_accuracy_list_by_rep_32_3B.png')\n",
    "plot_score_layers(model_name_32_3B_list, range(model_32_3B.config.num_hidden_layers+1), f1_list_by_rep, 'f1',  'F1(macro)', 'meta-rep classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_3B_list[0]}_f1_list_by_rep_32_3B.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classification_report, classification_report_name in zip([classification_report_list_for3, classification_report_list_for2, classification_report_list_for_meta, classification_report_list_by_rep], ['classification_report_list_for3', 'classification_report_list_for2', 'classification_report_list_for_meta', 'classification_report_list_by_rep']):\n",
    "    for model_name in model_name_32_3B_list:\n",
    "        path_classification_report = f'/home/s2420422/metano2/result/eval/classification_report_{model_name}.txt'\n",
    "        with open(path_classification_report, mode='w') as f:\n",
    "            for i, l in enumerate(classification_report[model_name]):\n",
    "                print(i, file=f)\n",
    "                print(l, file=f)\n",
    "                print('---', file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_name_32_3B_list:\n",
    "    print(classification_report_list_for3[model_name][28], classification_report_list_for2[model_name][28], classification_report_list_for_meta[model_name][28], classification_report_list_by_rep[model_name][28], sep='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c09b78",
   "metadata": {},
   "source": [
    "### cross test 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_0_per_layer, accuracy_list_1_per_layer, accuracy_list_2_per_layer = defaultdict(dict),defaultdict(dict),defaultdict(dict)\n",
    "for layer in range(17):\n",
    "    train_test_split_list = []\n",
    "    for hidden_state_ans_per_layer_dict in hidden_state_ans_per_layer_dict_32_1Blist:\n",
    "        train_test_split_list.append(train_test_split_from_hidden_state_ans_per_layer_dict(hidden_state_ans_per_layer_dict, layer, token_position=-1, ans_for3=True, test_size=0.25, random_state=25))\n",
    "    accuracy_list_0 = eval_probe_crosstest(model_name_32_1B_list[0], train_test_split_list[0], train_test_split_list[1:], model_name_32_1B_list[1:])\n",
    "    accuracy_list_1 = eval_probe_crosstest(model_name_32_1B_list[1], train_test_split_list[1], [train_test_split_list[0], train_test_split_list[2]], [model_name_32_1B_list[0],model_name_32_1B_list[2]])\n",
    "    accuracy_list_2 = eval_probe_crosstest(model_name_32_1B_list[2], train_test_split_list[2], train_test_split_list[0:2], model_name_32_1B_list[0:2])\n",
    "\n",
    "    accuracy_list_0_per_layer[layer] = accuracy_list_0\n",
    "    accuracy_list_1_per_layer[layer] = accuracy_list_1\n",
    "    accuracy_list_2_per_layer[layer] = accuracy_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_crosstest(accuracy_list_per_layer=accuracy_list_0_per_layer, layer_start=12, layer_end=16, y_label='', title='head prover', output_path='')\n",
    "plot_score_crosstest(accuracy_list_per_layer=accuracy_list_1_per_layer, layer_start=12, layer_end=16, y_label='', title='tail prover', output_path='')\n",
    "plot_score_crosstest(accuracy_list_per_layer=accuracy_list_2_per_layer, layer_start=12, layer_end=16, y_label='', title='middle prover', output_path='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_0_per_layer1, accuracy_list_1_per_layer1, accuracy_list_2_per_layer1 = defaultdict(dict),defaultdict(dict),defaultdict(dict)\n",
    "for layer in range(17):\n",
    "    train_test_split_list = []\n",
    "    for hidden_state_ans_per_layer_dict in hidden_state_ans_per_layer_dict_32_1Blist1:\n",
    "        train_test_split_list.append(train_test_split_from_hidden_state_ans_per_layer_dict(hidden_state_ans_per_layer_dict, layer, token_position=-1, ans_for3=True, test_size=0.25, random_state=25))\n",
    "    accuracy_list_0 = eval_probe_crosstest(model_name_32_1B_list1[0], train_test_split_list[0], train_test_split_list[1:], model_name_32_1B_list1[1:])\n",
    "    accuracy_list_1 = eval_probe_crosstest(model_name_32_1B_list1[1], train_test_split_list[1], [train_test_split_list[0], train_test_split_list[2]], [model_name_32_1B_list1[0],model_name_32_1B_list1[2]])\n",
    "    accuracy_list_2 = eval_probe_crosstest(model_name_32_1B_list1[2], train_test_split_list[2], train_test_split_list[0:2], model_name_32_1B_list1[0:2])\n",
    "\n",
    "    accuracy_list_0_per_layer1[layer] = accuracy_list_0\n",
    "    accuracy_list_1_per_layer1[layer] = accuracy_list_1\n",
    "    accuracy_list_2_per_layer1[layer] = accuracy_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a7fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_crosstest(accuracy_list_per_layer=accuracy_list_0_per_layer1, layer_start=12, layer_end=16, y_label='', title='head prover', output_path='')\n",
    "plot_score_crosstest(accuracy_list_per_layer=accuracy_list_1_per_layer1, layer_start=12, layer_end=16, y_label='', title='tail prover', output_path='')\n",
    "plot_score_crosstest(accuracy_list_per_layer=accuracy_list_2_per_layer1, layer_start=12, layer_end=16, y_label='', title='middle prover', output_path='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b033c7b2",
   "metadata": {},
   "source": [
    "### not cross test 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d253c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tp in [-1]:\n",
    "    accuracy_list_for3_32_1B, accuracy_list_for2_32_1B, f1_list_for3_32_1B, _ = \\\n",
    "        apply_probe_to_hidden_states_test(model_name_32_1B_list, 17, hidden_state_ans_per_layer_dict_32_1Blist, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_meta_reps=['famous', 'understood','unrecognized', 'unfamiliar',  'biased', 'relevant']\n",
    "test_target_meta_reps=['known','unknown','boring']\n",
    "for tp in [-1]:\n",
    "    accuracy_list_for3_32_1B1, accuracy_list_for2_32_1B1, accuracy_list_for_meta_32_1B1, accuracy_list_by_rep_32_1B1, f1_list_for3_32_1B1, _, f1_list_for_meta_32_1B1, f1_list_by_rep_32_1B1, classification_report_list_for3, classification_report_list_for2, classification_report_list_for_meta, classification_report_list_by_rep = \\\n",
    "        apply_probe_to_hidden_states_test(model_name_list=model_name_32_1B_list1, layer_num=17, hidden_state_ans_per_layer_dict_list=hidden_state_ans_per_layer_dict_32_1Blist1, token_position=tp, target_flag=True, train_target_meta_reps=train_target_meta_reps, test_target_meta_reps=test_target_meta_reps)\n",
    "plot_score_layers(model_name_32_1B_list1, range(17), accuracy_list_for2_32_1B1, 'accuracy', 'Accuracy', 'binary(known, unknown) classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_1B_list1[0]}_{\"-\".join(train_target_meta_reps)}{\"-\".join(test_target_meta_reps)}_accuracy_list_for2_32_1B.png')\n",
    "plot_score_layers(model_name_32_1B_list1, range(17), accuracy_list_for3_32_1B1, 'accuracy',  'Accuracy', 'meta+none classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_1B_list1[0]}_{\"-\".join(train_target_meta_reps)}{\"-\".join(test_target_meta_reps)}_accuracy_list_for3_32_1B_.png')\n",
    "plot_score_layers(model_name_32_1B_list1, range(17), f1_list_for3_32_1B1, 'f1',  'F1(macro)', 'meta+none classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_1B_list1[0]}_{\"-\".join(train_target_meta_reps)}{\"-\".join(test_target_meta_reps)}_f1_list_for3_32_1B.png')\n",
    "plot_score_layers(model_name_32_1B_list1, range(17), accuracy_list_for_meta_32_1B1, 'accuracy',  'Accuracy', 'meta classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_1B_list1[0]}_{\"-\".join(train_target_meta_reps)}{\"-\".join(test_target_meta_reps)}_accuracy_list_for3_32_1B.png')\n",
    "plot_score_layers(model_name_32_1B_list1, range(17), f1_list_for_meta_32_1B1, 'f1',  'F1(macro)', 'meta classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_1B_list1[0]}_{\"-\".join(train_target_meta_reps)}{\"-\".join(test_target_meta_reps)}_f1_list_for3_32_1B.png')\n",
    "plot_score_layers(model_name_32_1B_list1, range(17), accuracy_list_by_rep_32_1B1, 'accuracy',  'Accuracy', 'meta-rep classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_1B_list1[0]}_accuracy_list_by_rep_32_1B.png')\n",
    "plot_score_layers(model_name_32_1B_list1, range(17), f1_list_by_rep_32_1B1, 'f1',  'F1(macro)', 'meta-rep classification',  f'/home/s2420422/metano2/result/eval/{model_name_32_1B_list1[0]}_f1_list_by_rep_32_1B.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_name_32_1B_list1:\n",
    "    print(classification_report_list_for3[model_name][16], classification_report_list_for2[model_name][16], classification_report_list_for_meta[model_name][16], classification_report_list_by_rep[model_name][16], sep='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_across_layers(model_name_list, layer_list, score_list_for3, score_list_for2, y_label, title, output_path):\n",
    "    # Plot score values across layers with distinct styles for clarity\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    linestyle_l = [\"solid\", \"dashed\", \"dashdot\", \"dotted\"]\n",
    "    color_l = ['r','y','b','m']\n",
    "\n",
    "    for i, (model_name, score_list) in enumerate(zip([model_name_list]*2, [score_list_for2, score_list_for3])):\n",
    "\n",
    "        plt.plot(\n",
    "            layer_list,\n",
    "            score_list[model_name],\n",
    "            marker='o',\n",
    "            color=color_l[i],\n",
    "            #linestyle=linestyle_l[i],\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            label=model_name\n",
    "        )\n",
    "\n",
    "    \n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    #plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    #plt.savefig(output_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb82813",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tp in [-1]:    \n",
    "    accuracy_list_for3_32_3B, accuracy_list_for2_32_3B = \\\n",
    "        apply_probe_to_hidden_states_test(model_name_32_3B_list, 29, hidden_state_ans_per_layer_dict_32_3Blist, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c22c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for3 = '/home/s2420422/metano2/result/accuracy_list_for3_32_1B_01.txt'\n",
    "path_for2 = '/home/s2420422/metano2/result/accuracy_list_for2_32_1B_01.txt'\n",
    "with open(path_for3, mode='w') as f:\n",
    "    for k, v in accuracy_list_for3_32_1B.items():\n",
    "        print(f'{k}\\n', file=f)\n",
    "        for i in range(len(v)):\n",
    "            print(f'layer:{i} {v[i]}', file=f)\n",
    "        print(f'\\n\\n\\n', file=f)\n",
    "\n",
    "with open(path_for2, mode='w') as f:\n",
    "    for k, v in accuracy_list_for2_32_1B.items():\n",
    "        print(f'{k}\\n', file=f)\n",
    "        for i in range(len(v)):\n",
    "            print(f'layer:{i} {v[i]}', file=f)\n",
    "        print(f'\\n\\n\\n', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tp in [-1]:\n",
    "    real_tp = tp+1\n",
    "    output_path = f\"../figs/probe/r2_score_token{real_tp}_01.png\"\n",
    "\n",
    "    plot_score_across_layers(\n",
    "        model_name_32_1B_list,\n",
    "        range(17), # 1B:emb+16, 3B:emb+18\n",
    "        accuracy_list_for3_32_1B[model_name_32_1B_list[0]], accuracy_list_for2_32_1B[model_name_32_1B_list[1]],\n",
    "        'Accuracy', # y_label\n",
    "        f'probe_{real_tp}th_token', # title of the plot\n",
    "        output_path=output_path\n",
    "        )\n",
    "\n",
    "    logger.info(f\"figure saved to {output_path}\")\n",
    "\n",
    "\n",
    "    output_path = f\"../figs/probe/accuracy_token{real_tp}_01.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6629f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tp in [-1]:\n",
    "    real_tp = tp+1\n",
    "    output_path = f\"../figs/probe/accuracy_token{real_tp}_01.png\"\n",
    "\n",
    "    plot_score_across_layers(\n",
    "        model_name_323B_list,\n",
    "        range(29), # emb+ \n",
    "        accuracy_list_for3_323B[model_name_323B_list[0]], accuracy_list_for2_323B[model_name_323B_list[1]],\n",
    "        'Accuracy', # y_label\n",
    "        f'probe_{real_tp}th_token', # title of the plot\n",
    "        output_path=output_path\n",
    "        )\n",
    "\n",
    "    logger.info(f\"figure saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fe6b4",
   "metadata": {},
   "source": [
    "### prover 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SingleLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size) \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_1DNN(model, train_loader, valid_loader, epoch_num, lambda_l1=0.01, save_dir='../logs/'):\n",
    "\n",
    "    writer = SummaryWriter(log_dir=save_dir)\n",
    "\n",
    "    #input_size = 2048 output_size = 3\n",
    "    criterion = nn.CrossEntropyLoss()  # 多クラス分類用の損失関数\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    softmax = nn.Softmax(dim=1) # predict用\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss_list = []\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for inputs, targets in (train_loader):\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            loss = criterion(outputs, targets) \n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            loss += lambda_l1 * l1_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_list.append(loss.item()) \n",
    "\n",
    "            _, predicted = torch.max(softmax(outputs), 1)\n",
    "            \n",
    "            predicted_classes = torch.argmax(predicted, dim=1)\n",
    "            target_classes = torch.argmax(targets, dim=1)\n",
    "            correct_tensor = (predicted_classes == target_classes)\n",
    "            total_train_correct += correct_tensor.sum().item()\n",
    "            total_train_samples += targets.size(0)\n",
    "        \n",
    "        train_accuracy = 100.0 * total_train_correct / total_train_samples\n",
    "        writer.add_scalar('train_accuracy', train_accuracy, epoch)\n",
    "        mean_train_loss = np.mean(train_loss_list)\n",
    "        writer.add_scalar('mean_train_loss', mean_train_loss, epoch)\n",
    "        logger.info(f\"Epoch {epoch+1}/{epoch_num}: Train Loss: {mean_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        valid_loss_list = []\n",
    "        total_valid_correct = 0\n",
    "        total_valid_samples = 0\n",
    "        \n",
    "        for inputs, targets in (valid_loader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets) \n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss += lambda_l1 * l1_norm\n",
    "            \n",
    "            valid_loss_list.append(loss.item()) \n",
    "            _, predicted = torch.max(softmax(outputs), 1)\n",
    "\n",
    "            predicted_classes = torch.argmax(predicted, dim=1)\n",
    "            target_classes = torch.argmax(targets, dim=1)\n",
    "            correct_tensor = (predicted_classes == target_classes)\n",
    "            total_valid_correct += correct_tensor.sum().item()\n",
    "            total_valid_samples += targets.size(0)\n",
    "\n",
    "        valid_accuracy = 100.0 * total_valid_correct / total_valid_samples\n",
    "        writer.add_scalar('valid_accuracy', valid_accuracy, epoch)\n",
    "        mean_valid_loss = np.mean(valid_loss_list)\n",
    "        writer.add_scalar('mean_valid_loss', mean_valid_loss, epoch)\n",
    "        logger.info(f\"Epoch {epoch+1}/{epoch_num}: Valid Loss: {mean_valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babee9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "enc = preprocessing.OneHotEncoder(sparse=False)\n",
    "\n",
    "labels = ['none', 'known', 'unknown']\n",
    "enc.fit(np.array(labels).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e690f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_ans = hidden_state_ans_per_layer_dict_32_1B_fix_head[16]\n",
    "Xs, ys = [], []\n",
    "for dic in hidden_state_ans:\n",
    "    Xs.append(torch.tensor(dic['hidden_states']))\n",
    "    ys.append(dic['meta_tag'])\n",
    "    #ys.append(torch.tensor(enc.transform([dic['meta_tag']])))\n",
    "Xs = pad_sequence(Xs, batch_first=True)\n",
    "ys = torch.tensor(enc.transform(pd.DataFrame(ys))).long()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(Xs, ys, test_size=0.2, random_state=42)\n",
    "\n",
    "# TensorDataset に変換\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "\n",
    "# DataLoader を作成\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0dfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, output_size = 2048, 3\n",
    "model = SingleLayerNN(input_size, output_size)\n",
    "train_1DNN(model=model, train_loader=train_loader, valid_loader=valid_loader, epoch_num=300, lambda_l1=0.1, save_dir='/home/s2420422/metano2/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b95b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_1DNN_to_hidden_states_test(\n",
    "    model_name_list, layer_num, hidden_state_ans_per_layer_dict_list, token_position=-1\n",
    "):\n",
    "    r2_score_list, accuracy_list = defaultdict(list), defaultdict(list)\n",
    "    for model_name, hidden_state_ans_per_layer_dict in zip(model_name_list, hidden_state_ans_per_layer_dict_list):\n",
    "        for layer in range(layer_num):\n",
    "            hidden_states_list, ans_list = list(), list()\n",
    "            hidden_state_ans_per_layer = hidden_state_ans_per_layer_dict[layer]\n",
    "            for dic in hidden_state_ans_per_layer:\n",
    "                hidden_states_list.append(dic['hidden_states'][token_position]) # input_token最終 (emb_size)\n",
    "                ans_list.append(dic['meta_tag'])\n",
    "\n",
    "                \n",
    "            '''\n",
    "            # Encode labels for classification\n",
    "            label_encoder = LabelEncoder()\n",
    "            encoded_labels = label_encoder.fit_transform(labels)\n",
    "            '''\n",
    "            \n",
    "            # Split data into training and testing sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                hidden_states_list, \n",
    "                ans_list, \n",
    "                test_size=0.25, \n",
    "                random_state=RANDOM_SEED\n",
    "            )\n",
    "            #print(f'train:{y_train}, X_train:{X_train}')\n",
    "            \n",
    "            # Scale data based on training data\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Fit a Logistic Regression model\n",
    "            clf = LogisticRegression(max_iter=200)\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Calculate R^2 score and accuracyon test data\n",
    "            r2_test = 1#r2_score(y_test, clf.predict(X_test_scaled))\n",
    "            r2_score_list[model_name].append(r2_test)\n",
    "            accuracy_test = accuracy_score(y_test, clf.predict(X_test_scaled))\n",
    "            accuracy_list[model_name].append(accuracy_test)\n",
    "            \n",
    "            # Log important metrics\n",
    "            logger.info(f\"token_position:{token_position},Layer: {layer}, Test R^2: {r2_test}, Test Accuracy: {accuracy_test}\")\n",
    "            #logger.info(f\"Model: {model}, Layer: {layer}, Test R^2: {r2_test}, Test Accuracy: {accuracy_test}\")\n",
    "            \n",
    "    return r2_score_list, accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510684de",
   "metadata": {},
   "source": [
    "### 可視化 t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_ans_per_layer_dict_32_1B_fix_head = inference_and_collect_hidden_states_test(model_32_1B_fix_head, tokenizer_32_1B, dd_fix_head['train'],  len(dd_fix_head['train']),)\n",
    "hidden_state_ans_per_layer_dict_32_1B_fix_tail = inference_and_collect_hidden_states_test(model_32_1B_fix_tail, tokenizer_32_1B, dd_fix_tail['train'],  len(dd_fix_tail['train']),)\n",
    "hidden_state_ans_per_layer_dict_32_1B_fix_middle = inference_and_collect_hidden_states_test(model_32_1B_fix_middle, tokenizer_32_1B, dd_fix_middle['train'],  len(dd_fix_middle['train']),)\n",
    "\n",
    "hidden_state_ans_per_layer_dict_32_3B_fix_head = inference_and_collect_hidden_states_test(model_32_3B_fix_head, tokenizer_32_3B, dd_fix_head['train'],  len(dd_fix_head['train']),)\n",
    "hidden_state_ans_per_layer_dict_32_3B_fix_tail = inference_and_collect_hidden_states_test(model_32_3B_fix_tail, tokenizer_32_3B, dd_fix_tail['train'],  len(dd_fix_tail['train']),)\n",
    "hidden_state_ans_per_layer_dict_32_3B_fix_middle = inference_and_collect_hidden_states_test(model_32_3B_fix_middle, tokenizer_32_3B, dd_fix_middle['train'],  len(dd_fix_middle['train']),)\n",
    "\n",
    "model_name_321B_list = ['model_32_1B_fix_head', 'model_32_1B_fix_tail', 'model_32_1B_fix_middle']\n",
    "model_name_323B_list = ['model_32_3B_fix_head', 'model_32_3B_fix_tail', 'model_32_3B_fix_middle']\n",
    "\n",
    "hidden_state_ans_per_layer_dict_32_1Blist = \\\n",
    "    [hidden_state_ans_per_layer_dict_32_1B_fix_head, hidden_state_ans_per_layer_dict_32_1B_fix_tail, hidden_state_ans_per_layer_dict_32_1B_fix_middle]\n",
    "hidden_state_ans_per_layer_dict_32_3Blist = \\\n",
    "    [hidden_state_ans_per_layer_dict_32_3B_fix_head, hidden_state_ans_per_layer_dict_32_3B_fix_tail, hidden_state_ans_per_layer_dict_32_3B_fix_middle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a992c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def apply_tsne_to_hidden_states_and_visualise(\n",
    "    hidden_states_l, ans_l, title, output_path, n_components=2, perplexity=30, learning_rate=200,\n",
    "):\n",
    "\n",
    "    # Scale data before applying t-SNE\n",
    "    scaling = StandardScaler()\n",
    "    Scaled_data = scaling.fit_transform(hidden_states_l)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, learning_rate=learning_rate, random_state=RANDOM_SEED)\n",
    "    x = tsne.fit_transform(Scaled_data)\n",
    "\n",
    "    '''\n",
    "    # 再度小さく\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) # TODO サイズは調整\n",
    "    x = scaler.fit_transform(x)\n",
    "    print(len(x))\n",
    "    '''\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    color_map = {\n",
    "        \"none\": \"#0072B2\",  # Dark Blue\n",
    "        \"known\": \"#E69F00\",  # Orange\n",
    "        \"unknown\": \"#CC79A7\",  # Purple\n",
    "    }\n",
    "    marker_map = { # https://matplotlib.org/stable/api/markers_api.html\n",
    "        \"none\": '_', \n",
    "        \"known\": 'o',\n",
    "        \"unknown\": 'x', \n",
    "    }\n",
    "\n",
    "    # Separate the data by label\n",
    "    for meta_tag in [\"none\", \"known\", \"unknown\"]:\n",
    "        indices = [i for i, x in enumerate(ans_l) if x == meta_tag]\n",
    "        plt.scatter(\n",
    "            x[indices, 0],\n",
    "            x[indices, 1],\n",
    "            color=color_map[meta_tag],\n",
    "            marker=marker_map[meta_tag],\n",
    "            label=meta_tag,\n",
    "            alpha=0.5\n",
    "        )  # Square marker for 'ooo'\n",
    "\n",
    "\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.xlabel(\"t-SNE dimension 1\")\n",
    "    plt.ylabel(\"t-SNE dimension 2\")\n",
    "    plt.legend(title=title)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59eff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_state_ans_dict_per_layer, model_name in zip(hidden_state_ans_per_layer_dict_32_1Blist1, model_name_32_1B_list1):\n",
    "    layer_num = 17\n",
    "\n",
    "    with tqdm(range(layer_num)) as layer_epoch:\n",
    "        for layer in layer_epoch:\n",
    "            hidden_states_l, ans_l = list(), list()\n",
    "            for hidden_state_ans_dict in hidden_state_ans_dict_per_layer[layer]:\n",
    "                hidden_states_l.append(hidden_state_ans_dict['hidden_states'][-1]) # token_position=-1\n",
    "                ans_l.append(hidden_state_ans_dict['meta_tag'])\n",
    "\n",
    "            for n_components_size in [2, 3]:\n",
    "                title = f't-SNE_{model_name}_layer{layer}_{n_components_size}'\n",
    "                output_path = f'/home/s2420422/metano2/result/eval/t-SNE_{model_name}_layer{layer}_{n_components_size}'\n",
    "\n",
    "                apply_tsne_to_hidden_states_and_visualise(hidden_states_l=hidden_states_l, ans_l=ans_l, title=title, output_path=output_path, n_components=n_components_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb49a8",
   "metadata": {},
   "source": [
    "### 可視化 UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_umap_to_hidden_states_and_visualise(\n",
    "    hidden_states_l, ans_l, title, output_path, n_components=2, n_neighbors=15, min_dist=0.1\n",
    "):\n",
    "    # Scale data before applying UMAP\n",
    "    scaling = StandardScaler()\n",
    "    Scaled_data = scaling.fit_transform(hidden_states_l)\n",
    "\n",
    "    # Apply UMAP\n",
    "    umap = UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    x = umap.fit_transform(Scaled_data)\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    color_map = {\n",
    "        \"none\": \"#0072B2\",  # Dark Blue\n",
    "        \"known\": \"#E69F00\",  # Orange\n",
    "        \"unknown\": \"#CC79A7\",  # Purple\n",
    "    }\n",
    "    marker_map = { # https://matplotlib.org/stable/api/markers_api.html\n",
    "        \"none\": '_', \n",
    "        \"known\": 'o',\n",
    "        \"unknown\": 'x', \n",
    "    }\n",
    "\n",
    "    # Separate the data by label\n",
    "    for meta_tag in [\"none\", \"known\", \"unknown\"]:\n",
    "        indices = [i for i, x in enumerate(ans_l) if x == meta_tag]\n",
    "        plt.scatter(\n",
    "            x[indices, 0],\n",
    "            x[indices, 1],\n",
    "            color=color_map[meta_tag],\n",
    "            marker=marker_map[meta_tag],\n",
    "            label=meta_tag,\n",
    "            alpha=0.5\n",
    "        ) \n",
    "\n",
    "\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.xlabel(\"UMAP dimension 1\")\n",
    "    plt.ylabel(\"UMAP dimension 2\")\n",
    "    plt.legend(title=title)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_state_ans_dict_per_layer, model_name in zip(hidden_state_ans_per_layer_dict_32_1Blist1, model_name_32_1B_list1):\n",
    "    layer_num = 17\n",
    "\n",
    "    with tqdm(range(layer_num)) as layer_epoch:\n",
    "        for layer in layer_epoch:\n",
    "            hidden_states_l, ans_l = list(), list()\n",
    "            for hidden_state_ans_dict in hidden_state_ans_dict_per_layer[layer]:\n",
    "                hidden_states_l.append(hidden_state_ans_dict['hidden_states'][-1]) # token_position=-1\n",
    "                ans_l.append(hidden_state_ans_dict['meta_tag'])\n",
    "\n",
    "            for n_components_size in [2]:\n",
    "                title = f'umap_{model_name}_layer{layer}_{n_components_size}'\n",
    "                output_path = f'/home/s2420422/metano2/result/eval/umap_{model_name}_layer{layer}_{n_components_size}'\n",
    "\n",
    "                apply_umap_to_hidden_states_and_visualise(hidden_states_l=hidden_states_l, ans_l=ans_l, title=title, output_path=output_path, n_components=n_components_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
